{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044d9c6a",
   "metadata": {},
   "source": [
    "# 2025 데이터 크리에이터 캠프\n",
    "\n",
    "@PHASE: Mission 4-2\n",
    "\n",
    "@TEAM: 최후의 인공지능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5ea4cfae13401",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability\n",
    "\n",
    "- GPU 번호 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e888eba8b18b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 25 18:30:47 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.4     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              63W / 300W |  63706MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000000:65:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              43W / 300W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          On  | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              42W / 300W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          On  | 00000000:E3:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              44W / 300W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efa704fa81e6072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 1\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066676dc",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "- 의존성 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b08ea64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n"
     ]
    }
   ],
   "source": [
    "from os import path, makedirs\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.utils.notebook import NotebookProgressBar\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f497f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916fe787",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()  # 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9215d175116b3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbrew\u001b[0m (\u001b[33mbrew-research\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/elicer/K-ICT_DataCreatorCamp_2025_PVT/src/wandb/run-20251125_183055-e9s6txcl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brew-research/Mission_4-2/runs/e9s6txcl' target=\"_blank\">UNet-50p</a></strong> to <a href='https://wandb.ai/brew-research/Mission_4-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/brew-research/Mission_4-2' target=\"_blank\">https://wandb.ai/brew-research/Mission_4-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/brew-research/Mission_4-2/runs/e9s6txcl' target=\"_blank\">https://wandb.ai/brew-research/Mission_4-2/runs/e9s6txcl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/brew-research/Mission_4-2/runs/e9s6txcl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7efa8f8f42f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_NAME = \"Mission_4-2\"\n",
    "RUN_NAME = \"UNet-50p\"\n",
    "\n",
    "# WandB Initialization\n",
    "wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e1c723f76fc4f",
   "metadata": {},
   "source": [
    "## 3. Define Dataset\n",
    "\n",
    "- 데이터셋 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4212f",
   "metadata": {},
   "source": [
    "### 3.1. 데이터셋 홀더 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f77a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetHolder:\n",
    "    train: Dataset = None\n",
    "    valid: Dataset = None\n",
    "    test: Dataset = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        print(f\"INFO: Dataset loaded successfully. Number of samples - \", end='')\n",
    "        if self.train:\n",
    "            print(f\"Train: {len(self.train)}\", end='')\n",
    "        if self.valid:\n",
    "            if self.train: print(', ', end='')\n",
    "            print(f\"Valid: {len(self.valid)}\", end='')\n",
    "        if self.test:\n",
    "            if self.train: print(', ', end='')\n",
    "            print(f\"Test: {len(self.test)}\", end='')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataLoaderHolder:\n",
    "    train: object = None\n",
    "    valid: object = None\n",
    "    test: object = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7375653",
   "metadata": {},
   "source": [
    "### 3.2. 데이터셋 클래스 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d49f00",
   "metadata": {},
   "source": [
    "#### 3.2.1. 데이터셋 인덱스 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774e51d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "# Github Release URL for datasets\n",
    "# This will be removed after the contest ends due to the copyright issue.\n",
    "base_git_path = \"https://github.com/b-re-w/K-ICT_DataCreatorCamp_2025/releases/download/dt/\"\n",
    "\n",
    "\n",
    "class KompsatIndex(Enum):\n",
    "    TRAIN = \"TS_KS.zip\"\n",
    "    VALID = \"VS_KS.zip\"\n",
    "    TRAIN_BBOX = \"TL_KS_BBOX.zip\"\n",
    "    VALID_BBOX = \"VL_KS_BBOX.zip\"\n",
    "    TRAIN_LINE = \"TL_KS_LINE.zip\"\n",
    "    VALID_LINE = \"VL_KS_LINE.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\"\n",
    "\n",
    "\n",
    "class SentinelIndex(Enum):\n",
    "    TRAIN = \"TS_SN10_SN10.zip\"\n",
    "    VALID = \"VS_SN10_SN10.zip\"\n",
    "    TRAIN_MASK = \"TL_SN10.zip\"\n",
    "    VALID_MASK = \"VL_SN10.zip\"\n",
    "    TRAIN_GEMS = \"TS_SN10_GEMS.zip\"\n",
    "    VALID_GEMS = \"VS_SN10_GEMS.zip\"\n",
    "    TRAIN_AIR = \"TS_SN10_AIR_POLLUTION.zip\"\n",
    "    VALID_AIR = \"VS_SN10_AIR_POLLUTION.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\"\n",
    "\n",
    "    @property\n",
    "    def urls(self):\n",
    "        data_range = None\n",
    "        match self:\n",
    "            case SentinelIndex.TRAIN:\n",
    "                data_range = range(1, 9)\n",
    "            case SentinelIndex.TRAIN_GEMS:\n",
    "                data_range = range(1, 3)\n",
    "            case _:\n",
    "                return [self.url]\n",
    "        return [\n",
    "            self.url.replace(\".zip\", f\"_p{i}.zip\") for i in data_range\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def names(self):\n",
    "        data_range = None\n",
    "        match self:\n",
    "            case SentinelIndex.TRAIN:\n",
    "                data_range = range(1, 9)\n",
    "            case SentinelIndex.TRAIN_GEMS:\n",
    "                data_range = range(1, 3)\n",
    "            case _:\n",
    "                return [self.value]\n",
    "        return [\n",
    "            self.value.replace(\".zip\", f\"_p{i}.zip\") for i in data_range\n",
    "        ]\n",
    "\n",
    "\n",
    "class LandsatIndex(Enum):\n",
    "    TRAIN = \"TS_LS30_LS30.zip\"\n",
    "    VALID = \"VS_LS30_LS30.zip\"\n",
    "    TRAIN_MASK = \"TL_LS30.zip\"\n",
    "    VALID_MASK = \"VL_LS30.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\"\n",
    "\n",
    "    @property\n",
    "    def urls(self):\n",
    "        return [self.url]\n",
    "\n",
    "    @property\n",
    "    def names(self):\n",
    "        return [self.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020d1d7",
   "metadata": {},
   "source": [
    "#### 3.2.2. 목적별 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9706dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from os import path\n",
    "from glob import glob\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import VisionDataset, utils\n",
    "import rasterio\n",
    "\n",
    "from tqdm.asyncio import tqdm\n",
    "import concurrent.futures\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class DatasetModals(Enum):\n",
    "    RGB = \"rgb\"\n",
    "    NIR = \"nir\"\n",
    "    GEMS = \"gems\"\n",
    "    AIR = \"air\"\n",
    "\n",
    "\n",
    "class SentinelDataset(VisionDataset):\n",
    "    dataset_name = \"Sentinel\"\n",
    "\n",
    "    CLASSES = \"background\", \"industrial_area\"  # Class definitions: 0=background, 1=industrial_area\n",
    "    PALETTE = [0], [1]  # Grayscale palette | Background: black, Industrial area: white\n",
    "    ORIGINAL_PALETTE = [90, 90, 90], [10, 10, 10]  # Background: gray, Industrial area: black\n",
    "\n",
    "    DIRECTORIES = [\"images\", \"masks\", \"gems\", \"air\"]\n",
    "    DATA_LIST = [\n",
    "        SentinelIndex.TRAIN, SentinelIndex.VALID,\n",
    "        SentinelIndex.TRAIN_MASK, SentinelIndex.VALID_MASK,\n",
    "        SentinelIndex.TRAIN_GEMS, SentinelIndex.VALID_GEMS,\n",
    "        SentinelIndex.TRAIN_AIR, SentinelIndex.VALID_AIR,\n",
    "    ]\n",
    "    TRAIN_LIST = [SentinelIndex.TRAIN, SentinelIndex.TRAIN_MASK, SentinelIndex.TRAIN_GEMS, SentinelIndex.TRAIN_AIR]  # should be matched order with extract_dirs and valid_list\n",
    "    VALID_LIST = [SentinelIndex.VALID, SentinelIndex.VALID_MASK, SentinelIndex.VALID_GEMS, SentinelIndex.VALID_AIR]\n",
    "\n",
    "    @classmethod\n",
    "    async def download_method(cls, url, root, filename):\n",
    "        loop = asyncio.get_event_loop()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            await loop.run_in_executor(executor, utils.download_url, url, root, filename)\n",
    "\n",
    "    @classmethod\n",
    "    async def extract_method(cls, from_path, to_path):\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                await loop.run_in_executor(executor, utils.extract_archive, from_path, to_path)\n",
    "        except FileExistsError as e:\n",
    "            traceback.print_exc()\n",
    "            raise FileExistsError(str(e) + \"\\nPlease use Python 3.13 or later. 3.12 or earlier versions not support unzip over existing directory.\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path] = None,\n",
    "        train: bool = True,\n",
    "        data_type: DatasetModals | list[DatasetModals] = DatasetModals.RGB,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sentinel-2 dataset for semantic segmentation.\n",
    "        \n",
    "        Args:\n",
    "            root: Dataset root directory\n",
    "            train: True for training set, False for validation set\n",
    "            transforms: Joint transforms for image+mask\n",
    "            transform: Image transforms\n",
    "            target_transform: Mask transforms\n",
    "        \"\"\"\n",
    "        super().__init__(root, transforms=transforms, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(self.download(root))\n",
    "\n",
    "        self.types = data_type if isinstance(data_type, list) else [data_type]\n",
    "        self.root = path.join(root, self.dataset_name)\n",
    "        self.train = train\n",
    "        split = \"train\" if train else \"val\"\n",
    "        self.images, self.masks, self.gems, self.air = lists = [], [], [], []\n",
    "        extract_dirs = [path.join(self.root, anno, split) for anno in self.DIRECTORIES]\n",
    "        for lst, anno in zip(lists, self.DIRECTORIES):\n",
    "            lst.extend(sorted(glob(path.join(extract_dirs[self.DIRECTORIES.index(anno)], \"*.tif\"))))\n",
    "\n",
    "        assert len(self.images) == len(self.masks), \\\n",
    "            f\"Number of images ({len(self.images)}) and masks ({len(self.masks)}) do not match.\"\n",
    "        if DatasetModals.GEMS in self.types:\n",
    "            assert len(self.images) == len(self.gems), \\\n",
    "                f\"Number of images ({len(self.images)}) and GEMS data ({len(self.gems)}) do not match.\"\n",
    "        if DatasetModals.AIR in self.types:\n",
    "            assert len(self.images) == len(self.air), \\\n",
    "                f\"Number of images ({len(self.images)}) and AIR data ({len(self.air)}) do not match.\"\n",
    "\n",
    "        self.cached_data = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cached_data:\n",
    "            image, mask, gems, air = self.cached_data[idx]\n",
    "        else:\n",
    "            # Load image/mask using default_loader\n",
    "            image = self.load_raster(self.images[idx], channels=(1, 2, 3, 4) if DatasetModals.NIR in self.types else (1, 2, 3), normalize=True)\n",
    "            mask = self.load_raster(self.masks[idx])\n",
    "\n",
    "            # Convert mask to grayscale\n",
    "            mask = torch.where(mask == 10, 1, 0).to(torch.uint8)\n",
    "\n",
    "            # Load additional data if specified\n",
    "            gems, air = None, None\n",
    "            if DatasetModals.GEMS in self.types:\n",
    "                gems = self.load_raster(self.gems[idx], channels=range(1, 11), normalize=True)\n",
    "            if DatasetModals.AIR in self.types:\n",
    "                air = self.load_raster(self.air[idx], channels=range(1, 7), normalize=True)\n",
    "    \n",
    "            # Cache the loaded data\n",
    "            self.cached_data[idx] = (image, mask, gems, air)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            # Joint transforms (e.g., albumentations)\n",
    "            try:\n",
    "                image, mask, gems, air = self.transforms(image, mask, gems, air)\n",
    "            except Exception:\n",
    "                image, mask = self.transforms(image, mask)\n",
    "        else:\n",
    "            # Individual transforms\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            if self.target_transform:\n",
    "                try:\n",
    "                    mask, gems, air = self.target_transform(mask, gems, air)\n",
    "                except Exception:\n",
    "                    mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask, gems, air\n",
    "\n",
    "    def load_raster(self, path: Path, channels=(1,), normalize=False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Load TIF image using rasterio.\n",
    "    \n",
    "        Args:\n",
    "            path: Path to TIF file\n",
    "            channels: Channels to read (1-based indexing)\n",
    "            normalize: Whether to normalize the image to 0-1 range\n",
    "    \n",
    "        Returns:\n",
    "            Normalized image array in (C, H, W) format\n",
    "        \"\"\"\n",
    "        with rasterio.open(path) as src:\n",
    "            data = src.read(channels)\n",
    "\n",
    "        # Normalize to 0-1\n",
    "        if normalize:\n",
    "            data = data.astype(np.float32)\n",
    "            # Channel-wise Z-score normalization\n",
    "            for i in range(data.shape[0]):\n",
    "                channel_min = data[i].min()\n",
    "                channel_max = data[i].max()\n",
    "                diff = channel_max - channel_min\n",
    "                if diff != 0:\n",
    "                    data[i] = (data[i] - channel_min) / diff\n",
    "                else:\n",
    "                    data[i] = 0\n",
    "        return torch.from_numpy(data)\n",
    "\n",
    "    @classmethod\n",
    "    async def download(cls, root: str):\n",
    "        dataset_root = path.join(root, cls.dataset_name)\n",
    "        if path.exists(dataset_root):  # If the dataset directory already exists, skip download\n",
    "            return\n",
    "\n",
    "        print(f\"INFO: Downloading '{cls.dataset_name}' from server to {root}...\")\n",
    "        routines = []\n",
    "        for data in cls.DATA_LIST:\n",
    "            if path.isfile(path.join(root, data.value)):\n",
    "                print(f\"INFO: Dataset archive {data.value} found in the root directory. Skipping download.\")\n",
    "                continue\n",
    "\n",
    "            routines.extend(cls.download_method(url, root=root, filename=file) for url, file in zip(data.urls, data.names))\n",
    "        await tqdm.gather(*routines, desc=f\"Downloading {len(routines)} files\")\n",
    "\n",
    "        print(f\"INFO: Extracting '{cls.dataset_name}' dataset...\")\n",
    "        routines = []\n",
    "        as_train, as_valid = lambda d: path.join(d, \"train\"), lambda d: path.join(d, \"val\")\n",
    "        extract_dirs = [path.join(dataset_root, anno) for anno in cls.DIRECTORIES]\n",
    "        for trains, dirs in zip(cls.TRAIN_LIST, extract_dirs):\n",
    "            routines.extend(cls.extract_method(path.join(root, file), to_path=as_train(dirs)) for file in trains.names)\n",
    "        for valids, dirs in zip(cls.VALID_LIST, extract_dirs):\n",
    "            routines.extend(cls.extract_method(path.join(root, file), to_path=as_valid(dirs)) for file in valids.names)\n",
    "\n",
    "        await tqdm.gather(*routines, desc=f\"Extracting {len(routines)} files\")\n",
    "\n",
    "\n",
    "class SentinelDatasetForSegmentation(SentinelDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cea74794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class LandsatDataset(SentinelDataset):\n",
    "    dataset_name = \"Landsat\"\n",
    "\n",
    "    CLASSES = \"background\", \"urban_area\"  # Class definitions: 0=background, 1=urban_area\n",
    "    PALETTE = [0], [1]  # Grayscale palette | Background: black, Urban area: white\n",
    "    ORIGINAL_PALETTE = [90, 90, 90], [10, 10, 10]  # Background: gray, Urban area: black\n",
    "\n",
    "    DIRECTORIES = [\"images\", \"masks\"]\n",
    "    DATA_LIST = [\n",
    "        LandsatIndex.TRAIN, LandsatIndex.VALID,\n",
    "        LandsatIndex.TRAIN_MASK, LandsatIndex.VALID_MASK\n",
    "    ]\n",
    "    TRAIN_LIST = [LandsatIndex.TRAIN, LandsatIndex.TRAIN_MASK]  # should be matched order with extract_dirs and valid_list\n",
    "    VALID_LIST = [LandsatIndex.VALID, LandsatIndex.VALID_MASK]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path] = None,\n",
    "        train: bool = True,\n",
    "        data_type: DatasetModals | list[DatasetModals] = DatasetModals.RGB,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        types = [dt for dt in data_type if dt in (DatasetModals.RGB, DatasetModals.NIR)]\n",
    "        super().__init__(root, train, types, transforms, transform, target_transform)\n",
    "\n",
    "\n",
    "class LandsatDatasetForSegmentation(LandsatDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a0919",
   "metadata": {},
   "source": [
    "#### 3.2.3. 데이터셋 인스턴스 생성\n",
    "\n",
    "- Train, Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958226d",
   "metadata": {},
   "source": [
    "##### 3.2.3.1. 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "973243d0d0632803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dataset loaded successfully. Number of samples - Train: 8000, Valid: 1000\n",
      "\n",
      "INFO: Dataset loaded successfully. Number of samples - Train: 4000, Valid: 500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# 모달리티 설정\n",
    "modals = [DatasetModals.RGB, DatasetModals.NIR]\n",
    "\n",
    "sentinels = DatasetHolder(\n",
    "    train=SentinelDatasetForSegmentation(root=DATA_ROOT, train=True, data_type=modals),\n",
    "    valid=SentinelDatasetForSegmentation(root=DATA_ROOT, train=False, data_type=modals)\n",
    ")\n",
    "sentinels.test = sentinels.valid  # test set은 valid set과 동일\n",
    "\n",
    "landsats = DatasetHolder(\n",
    "    train=LandsatDatasetForSegmentation(root=DATA_ROOT, train=True, data_type=modals),\n",
    "    valid=LandsatDatasetForSegmentation(root=DATA_ROOT, train=False, data_type=modals)\n",
    ")\n",
    "landsats.test = landsats.valid  # test set은 valid set과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4368a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/Sentinel/images/train/SN10_CHN_00001_230409.tif',\n",
       " './data/Sentinel/masks/train/SN10_CHN_00001_230409.tif')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 경로 확인\n",
    "sentinels.train.images[0], sentinels.train.masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1973a6e3a7a4f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/K-ICT_DataCreatorCamp_2025_PVT/.venv/lib/python3.13/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0258, 0.0249, 0.0252,  ..., 0.1498, 0.1602, 0.1623],\n",
       "          [0.0245, 0.0251, 0.0261,  ..., 0.1383, 0.1697, 0.1835],\n",
       "          [0.0268, 0.0288, 0.0268,  ..., 0.1486, 0.1849, 0.1884],\n",
       "          ...,\n",
       "          [0.6881, 0.5596, 0.5427,  ..., 0.0554, 0.0900, 0.2466],\n",
       "          [0.7438, 0.6394, 0.6224,  ..., 0.0603, 0.1039, 0.2682],\n",
       "          [0.7989, 0.7212, 0.6401,  ..., 0.0653, 0.1316, 0.2929]],\n",
       " \n",
       "         [[0.0411, 0.0455, 0.0463,  ..., 0.1237, 0.1301, 0.1277],\n",
       "          [0.0385, 0.0397, 0.0416,  ..., 0.1216, 0.1433, 0.1440],\n",
       "          [0.0385, 0.0429, 0.0461,  ..., 0.1281, 0.1508, 0.1304],\n",
       "          ...,\n",
       "          [0.5874, 0.4572, 0.4633,  ..., 0.0555, 0.0712, 0.1858],\n",
       "          [0.6275, 0.5501, 0.5237,  ..., 0.0573, 0.0967, 0.2051],\n",
       "          [0.6872, 0.6254, 0.5583,  ..., 0.0575, 0.1243, 0.2078]],\n",
       " \n",
       "         [[0.1615, 0.1710, 0.1696,  ..., 0.2102, 0.2184, 0.2167],\n",
       "          [0.1664, 0.1686, 0.1686,  ..., 0.2056, 0.2311, 0.2321],\n",
       "          [0.1674, 0.1686, 0.1708,  ..., 0.2185, 0.2404, 0.2600],\n",
       "          ...,\n",
       "          [0.5593, 0.4895, 0.4534,  ..., 0.1821, 0.2015, 0.2661],\n",
       "          [0.6079, 0.5369, 0.5114,  ..., 0.1872, 0.2167, 0.2732],\n",
       "          [0.6678, 0.5901, 0.5441,  ..., 0.1844, 0.2227, 0.2787]],\n",
       " \n",
       "         [[0.8633, 0.8760, 0.8697,  ..., 0.5082, 0.5234, 0.5156],\n",
       "          [0.8760, 0.8760, 0.8673,  ..., 0.5168, 0.5362, 0.5290],\n",
       "          [0.8760, 0.8745, 0.8625,  ..., 0.5242, 0.5306, 0.4898],\n",
       "          ...,\n",
       "          [0.7921, 0.6817, 0.6425,  ..., 0.7847, 0.7057, 0.5360],\n",
       "          [0.8481, 0.7705, 0.7233,  ..., 0.7867, 0.6993, 0.5282],\n",
       "          [0.9224, 0.8481, 0.7569,  ..., 0.7943, 0.6623, 0.5284]]]),\n",
       " tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 출력 검증\n",
    "sentinels.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11feab2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentinels.train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2dc6e7",
   "metadata": {},
   "source": [
    "##### 3.2.3.2 출력 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d102cd1b5fdcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [0, 100, 1000]:\n",
    "    rgb_image, mask_image, _, _ = sentinels.train[idx]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(rgb_image.permute(1, 2, 0))\n",
    "    axes[0].set_title('Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(mask_image.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(\"Mask\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(axes[1].imshow(mask_image.squeeze(), cmap='gray'), ax=axes[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abedc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB 채널만 시각화\n",
    "for idx in [0, 100, 1000]:\n",
    "    rgb_image, mask_image, _, _ = sentinels.train[idx]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(rgb_image[:3, :, :].permute(1, 2, 0))\n",
    "    axes[0].set_title('Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(mask_image.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(\"Mask\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(axes[1].imshow(mask_image.squeeze(), cmap='gray'), ax=axes[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDVI 계산\n",
    "rgb_image, _, _, _ = sentinels.train[1000]\n",
    "nir, red = rgb_image[3, :, :], rgb_image[0, :, :]  # NIR 채널과 Red 채널 추출\n",
    "denominator = nir + red\n",
    "ndvi = torch.where(denominator > 0, (nir - red) / denominator, 0)\n",
    "\n",
    "print(\"NDVI 계산 완료!\")\n",
    "print(f\"NDVI 값 범위: {ndvi.min():.3f} ~ {ndvi.max():.3f}\")\n",
    "\n",
    "# NDVI 시각화\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(ndvi, cmap='RdYlGn')  # 빨강=낮음(식생 없음), 초록=높음(식생 풍부)\n",
    "plt.colorbar(label=\"NDVI\")\n",
    "plt.title(\"NDVI Map\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752cebd36e5c809",
   "metadata": {},
   "source": [
    "## 4. DataLoader\n",
    "\n",
    "- 데이터 로더 생성\n",
    "- A100 기준 배치 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d124eb51bc123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "PRETRAIN_BATCH_SIZE = 128, 500\n",
    "BATCH_SIZE = 64, 128, 128\n",
    "#PRETRAIN_BATCH_SIZE = 2, 8, 8\n",
    "#BATCH_SIZE = 2, 8, 8\n",
    "\n",
    "# Workers\n",
    "WORKERS = 32\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7255ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, masks, gems, air = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    masks = torch.stack(masks)\n",
    "    gems = torch.stack(gems) if gems[0] is not None else None\n",
    "    air = torch.stack(air) if air[0] is not None else None\n",
    "    return images, masks, gems, air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_loaders = DataLoaderHolder(\n",
    "    train=DataLoader(sentinels.train, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=WORKERS, collate_fn=collate_fn),\n",
    "    valid=DataLoader(sentinels.valid, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=WORKERS, collate_fn=collate_fn),\n",
    ")\n",
    "sentinel_loaders.test = sentinel_loaders.valid\n",
    "landsat_loaders = DataLoaderHolder(\n",
    "    train=DataLoader(landsats.train, batch_size=PRETRAIN_BATCH_SIZE[0], shuffle=True, num_workers=WORKERS, collate_fn=collate_fn),\n",
    "    valid=DataLoader(landsats.valid, batch_size=PRETRAIN_BATCH_SIZE[1], shuffle=False, num_workers=WORKERS, collate_fn=collate_fn)\n",
    ")\n",
    "landsat_loaders.test = landsat_loaders.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask(image, mask, alpha=0.4, color=(1,0,0)):\n",
    "    \"\"\"\n",
    "    image: (H,W,3) RGB\n",
    "    mask:  (H,W) binary mask (0 or 1)\n",
    "    alpha: 투명도\n",
    "    color: 오버레이 색상 (R,G,B)\n",
    "    \"\"\"\n",
    "    overlay = image.numpy().copy()\n",
    "    overlay[mask > 0.5] = (1-alpha)*overlay[mask > 0.5] + alpha*np.array(color)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "def visualize_overlay(X, Y, num_samples=3, set_name=\"Train\"):\n",
    "    idxs = random.sample(range(len(X)), num_samples)\n",
    "    plt.figure(figsize=(12, num_samples*4))\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        img = X[idx]\n",
    "        label = Y[idx]\n",
    "\n",
    "        # 원본\n",
    "        plt.subplot(num_samples, 3, i*3+1)\n",
    "        plt.imshow(img[:3, :, :].permute(1, 2, 0))\n",
    "        plt.title(f\"{set_name} Image {idx}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # 마스크\n",
    "        plt.subplot(num_samples, 3, i*3+2)\n",
    "        plt.imshow(label.squeeze(), cmap=\"gray\")\n",
    "        plt.title(\"Foreground Mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # 오버레이\n",
    "        overlayed = overlay_mask(img[:3, :, :].permute(1, 2, 0), label.squeeze())\n",
    "        plt.subplot(num_samples, 3, i*3+3)\n",
    "        plt.imshow(overlayed)\n",
    "        plt.title(\"Overlay\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 배치 데이터 시각화\n",
    "train_sample = next(iter(sentinel_loaders.train))\n",
    "valid_sample = next(iter(sentinel_loaders.valid))\n",
    "visualize_overlay(train_sample[0], train_sample[1], num_samples=2, set_name=\"Train\")\n",
    "visualize_overlay(valid_sample[0], valid_sample[1], num_samples=2, set_name=\"Valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4053c037f244da",
   "metadata": {},
   "source": [
    "## 5. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c344106",
   "metadata": {},
   "source": [
    "- 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "notebook_progress_callback",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookProgressCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_progress_bar = None\n",
    "        self.val_progress_bar = None\n",
    "        self.best_val_miou = 0.0\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        total_batches = len(trainer.train_dataloader)\n",
    "        self.train_progress_bar = NotebookProgressBar(\n",
    "            total_batches, prefix=f\"Training {trainer.current_epoch + 1}\",\n",
    "        )\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        metrics = trainer.callback_metrics\n",
    "        loss = metrics.get('train_loss_step', 0)\n",
    "        miou = metrics.get('train_miou_step', 0)\n",
    "        current_lr = trainer.optimizers[0].param_groups[0]['lr']\n",
    "        self.train_progress_bar.update(batch_idx+1, comment=f\"Loss={loss:.4f}, mIoU={miou:.4f}, LR={current_lr:.2e}\")\n",
    "\n",
    "    def on_validation_epoch_start(self, trainer, pl_module):\n",
    "        total_batches = len(trainer.val_dataloaders)\n",
    "        self.val_progress_bar = NotebookProgressBar(\n",
    "            total_batches, prefix=f\"Validating {trainer.current_epoch + 1}\"\n",
    "        )\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        total_batches = len(trainer.val_dataloaders)\n",
    "        if batch_idx < total_batches-1:\n",
    "            self.val_progress_bar.update(batch_idx+1, comment=\"\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_miou = metrics.get('val_miou', 0)\n",
    "        self.val_progress_bar.update(self.val_progress_bar.total, comment=f\"Loss={val_loss:.4f}, mIoU={val_miou:.4f}\")\n",
    "        if val_miou > self.best_val_miou:\n",
    "            self.best_val_miou = val_miou\n",
    "            print(f\"Best mIoU so far: {self.best_val_miou:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 모델 정의\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=\"resnet50\", encoder_weights=\"imagenet\",\n",
    "            in_channels=4, classes=1,\n",
    "            encoder_depth=5, decoder_channels=(512, 256, 128, 64, 64)\n",
    "        )\n",
    "\n",
    "        # 손실 함수 정의\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # mIoU 계산을 위한 변수\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def calculate_miou(self, preds, masks, threshold=0.5):\n",
    "        \"\"\"mIoU 계산 함수\"\"\"\n",
    "        preds = (torch.sigmoid(preds) > threshold).float()\n",
    "        masks = masks.float()\n",
    "\n",
    "        # Intersection and Union\n",
    "        intersection = (preds * masks).sum(dim=(1, 2, 3))\n",
    "        union = preds.sum(dim=(1, 2, 3)) + masks.sum(dim=(1, 2, 3)) - intersection\n",
    "\n",
    "        # IoU 계산 (union이 0인 경우 처리)\n",
    "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "        return iou.mean()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks, _, _ = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, masks.float())\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # mIoU 계산\n",
    "        miou = self.calculate_miou(outputs, masks)\n",
    "\n",
    "        # 로깅 - step과 epoch 둘 다\n",
    "        self.log('train_loss', loss, prog_bar=False, on_step=False, on_epoch=True)\n",
    "        self.log('train_loss_step', loss, prog_bar=False, on_step=True, on_epoch=False, batch_size=batch_size)\n",
    "        self.log('train_miou', miou, prog_bar=False, on_step=False, on_epoch=True)\n",
    "        self.log('train_miou_step', miou, prog_bar=False, on_step=True, on_epoch=False, batch_size=batch_size)\n",
    "\n",
    "        # WandB 로깅\n",
    "        wandb.log({\n",
    "            'train_loss': loss.item(), \n",
    "            'train_miou': miou.item(),\n",
    "            'step': self.global_step\n",
    "        })\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks, _, _ = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, masks.float())\n",
    "\n",
    "        # mIoU 계산\n",
    "        miou = self.calculate_miou(outputs, masks)\n",
    "\n",
    "        # 결과 저장\n",
    "        self.validation_step_outputs.append({\n",
    "            'val_loss': loss,\n",
    "            'val_miou': miou\n",
    "        })\n",
    "\n",
    "        return {'val_loss': loss, 'val_miou': miou}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # 에폭별 평균 계산\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in self.validation_step_outputs]).mean()\n",
    "        avg_miou = torch.stack([x['val_miou'] for x in self.validation_step_outputs]).mean()\n",
    "\n",
    "        # 로깅\n",
    "        self.log('val_loss', avg_loss, prog_bar=False)\n",
    "        self.log('val_miou', avg_miou, prog_bar=False)\n",
    "\n",
    "        # WandB 로깅\n",
    "        wandb.log({\n",
    "            'val_loss': avg_loss.item(),\n",
    "            'val_miou': avg_miou.item(),\n",
    "            'epoch': self.current_epoch\n",
    "        })\n",
    "\n",
    "        # 초기화\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, masks, _, _ = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, masks.float())\n",
    "\n",
    "        # mIoU 계산\n",
    "        miou = self.calculate_miou(outputs, masks)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_miou', miou)\n",
    "\n",
    "        return {'test_loss': loss, 'test_miou': miou}\n",
    "\n",
    "    def reset_optimizers_cache(self):\n",
    "        if hasattr(self, '_optim_dict_cache'):\n",
    "            del self._optim_dict_cache\n",
    "\n",
    "    def configure_optimizers(self, learning_rate=None, max_epochs=None):\n",
    "        if hasattr(self, '_optim_dict_cache'):\n",
    "            return self._optim_dict_cache\n",
    "        else:\n",
    "            self.hparams.learning_rate = learning_rate if learning_rate is not None else 1e-4\n",
    "            optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "            # Cosine Annealing Scheduler\n",
    "            self.hparams.max_epochs = max_epochs if max_epochs is not None else 50\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=self.hparams.max_epochs,\n",
    "                eta_min=1e-6\n",
    "            )\n",
    "\n",
    "            self._optim_dict_cache = {\n",
    "                'optimizer': optimizer,\n",
    "                'lr_scheduler': {\n",
    "                    'scheduler': scheduler,\n",
    "                    'interval': 'epoch',\n",
    "                    'frequency': 1\n",
    "                }\n",
    "            }\n",
    "\n",
    "            return self._optim_dict_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44373f47d600fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "set_seed(2025)\n",
    "model = SegmentationModel(learning_rate=1e-4)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfdb161ae65bd5",
   "metadata": {},
   "source": [
    "## 6. Train\n",
    "\n",
    "- Epoch 기반 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea451a",
   "metadata": {},
   "source": [
    "### 6.1. Landsat 데이터셋으로 프리트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48316cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "set_seed(2025)\n",
    "model.configure_optimizers(learning_rate=LEARNING_RATE, max_epochs=EPOCHS)\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=1, enable_progress_bar=False, callbacks=[NotebookProgressCallback()], enable_model_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5273c531e2c1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=landsat_loaders.train,\n",
    "    val_dataloaders=landsat_loaders.valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff911ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간 결과 저장\n",
    "makedirs(\"./results/\", exist_ok=True)\n",
    "save_path = f\"./results/mission_4__temp1.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"INFO: Final model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"results/mission_4__temp1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc74830",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsats.train.cached_data = {}\n",
    "landsats.valid.cached_data = {}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b88684",
   "metadata": {},
   "source": [
    "### 6.2. Sentinel 데이터셋으로 최종 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_optimizers_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19378ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "set_seed(2025)\n",
    "model.configure_optimizers(learning_rate=LEARNING_RATE, max_epochs=EPOCHS)\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=1, enable_progress_bar=False, callbacks=[NotebookProgressCallback()], enable_model_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf296fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=sentinel_loaders.train,\n",
    "    val_dataloaders=sentinel_loaders.valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 저장\n",
    "makedirs(\"./results/\", exist_ok=True)\n",
    "save_path = f\"./results/mission_4__temp2.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"INFO: Final model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0aace760f618d0",
   "metadata": {},
   "source": [
    "## 7. Evaluate\n",
    "\n",
    "- 최종 성능 확인 및 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5655e026216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Evaluation ===\")\n",
    "trainer.test(model, dataloaders=sentinel_loaders.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0e803b70cf08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(\"./results/\", exist_ok=True)\n",
    "save_path = f\"./results/mission_4-2.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"INFO: Final model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508ab0fd10e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정상 저장 확인\n",
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59c9498f5db854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creator-camp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
