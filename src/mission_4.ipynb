{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044d9c6a",
   "metadata": {},
   "source": [
    "# 2025 데이터 크리에이터 캠프\n",
    "\n",
    "@PHASE: Mission 4-2\n",
    "\n",
    "@TEAM: 최후의 인공지능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5ea4cfae13401",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability\n",
    "\n",
    "- GPU 번호 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e888eba8b18b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa704fa81e6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066676dc",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "- 의존성 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.utils.notebook import NotebookProgressBar\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fe787",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()  # 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215d175116b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"Mission_4\"\n",
    "RUN_NAME = \"UNet-1024\"\n",
    "\n",
    "# WandB Initialization\n",
    "wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e1c723f76fc4f",
   "metadata": {},
   "source": [
    "## 3. Define Dataset\n",
    "\n",
    "- 데이터셋 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4212f",
   "metadata": {},
   "source": [
    "### 3.1. 데이터셋 홀더 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetHolder:\n",
    "    train: Dataset = None\n",
    "    valid: Dataset = None\n",
    "    test: Dataset = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        print(f\"INFO: Dataset loaded successfully. Number of samples - \", end='')\n",
    "        if self.train:\n",
    "            print(f\"Train: {len(self.train)}\", end='')\n",
    "        if self.valid:\n",
    "            if self.train: print(', ', end='')\n",
    "            print(f\"Valid: {len(self.valid)}\", end='')\n",
    "        if self.test:\n",
    "            if self.train: print(', ', end='')\n",
    "            print(f\"Test: {len(self.test)}\", end='')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataLoaderHolder:\n",
    "    train: object = None\n",
    "    valid: object = None\n",
    "    test: object = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7375653",
   "metadata": {},
   "source": [
    "### 3.2. 데이터셋 클래스 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d49f00",
   "metadata": {},
   "source": [
    "#### 3.2.1. 데이터셋 인덱스 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e51d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "# Github Release URL for datasets\n",
    "# This will be removed after the contest ends due to the copyright issue.\n",
    "base_git_path = \"https://github.com/b-re-w/K-ICT_DataCreatorCamp_2025/releases/download/dt/\"\n",
    "\n",
    "\n",
    "class KompsatIndex(Enum):\n",
    "    TRAIN = \"TS_KS.zip\"\n",
    "    VALID = \"VS_KS.zip\"\n",
    "    TRAIN_BBOX = \"TL_KS_BBOX.zip\"\n",
    "    VALID_BBOX = \"VL_KS_BBOX.zip\"\n",
    "    TRAIN_LINE = \"TL_KS_LINE.zip\"\n",
    "    VALID_LINE = \"VL_KS_LINE.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\"\n",
    "\n",
    "\n",
    "class SentinelIndex(Enum):\n",
    "    TRAIN = \"TS_SN10_SN10.zip\"\n",
    "    VALID = \"VS_SN10_SN10.zip\"\n",
    "    TRAIN_MASK = \"TL_SN10.zip\"\n",
    "    VALID_MASK = \"VL_SN10.zip\"\n",
    "    TRAIN_GEMS = \"TS_SN10_GEMS.zip\"\n",
    "    VALID_GEMS = \"VS_SN10_GEMS.zip\"\n",
    "    TRAIN_AIR = \"TS_SN10_AIR_POLLUTION.zip\"\n",
    "    VALID_AIR = \"VS_SN10_AIR_POLLUTION.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\"\n",
    "\n",
    "    @property\n",
    "    def urls(self):\n",
    "        data_range = None\n",
    "        match self:\n",
    "            case SentinelIndex.TRAIN:\n",
    "                data_range = range(1, 9)\n",
    "            case SentinelIndex.TRAIN_GEMS:\n",
    "                data_range = range(1, 3)\n",
    "            case _:\n",
    "                return [self.url]\n",
    "        return [\n",
    "            self.url.replace(\".zip\", f\"_p{i}.zip\") for i in data_range\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def names(self):\n",
    "        data_range = None\n",
    "        match self:\n",
    "            case SentinelIndex.TRAIN:\n",
    "                data_range = range(1, 9)\n",
    "            case SentinelIndex.TRAIN_GEMS:\n",
    "                data_range = range(1, 3)\n",
    "            case _:\n",
    "                return [self.value]\n",
    "        return [\n",
    "            self.value.replace(\".zip\", f\"_p{i}.zip\") for i in data_range\n",
    "        ]\n",
    "\n",
    "\n",
    "class LandsatIndex(Enum):\n",
    "    TRAIN = \"TS_LS30_LS30.zip\"\n",
    "    VALID = \"VS_LS30_LS30.zip\"\n",
    "    TRAIN_MASK = \"TL_LS30.zip\"\n",
    "    VALID_MASK = \"VL_LS30.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\"\n",
    "\n",
    "    @property\n",
    "    def urls(self):\n",
    "        return [self.url]\n",
    "\n",
    "    @property\n",
    "    def names(self):\n",
    "        return [self.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020d1d7",
   "metadata": {},
   "source": [
    "#### 3.2.2. 목적별 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from os import path\n",
    "from glob import glob\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import VisionDataset, utils\n",
    "import rasterio\n",
    "\n",
    "from tqdm.asyncio import tqdm\n",
    "import concurrent.futures\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class DatasetModals(Enum):\n",
    "    RGB = \"rgb\"\n",
    "    NIR = \"nir\"\n",
    "    GEMS = \"gems\"\n",
    "    AIR = \"air\"\n",
    "\n",
    "\n",
    "class SentinelDataset(VisionDataset):\n",
    "    dataset_name = \"Sentinel\"\n",
    "\n",
    "    CLASSES = \"background\", \"industrial_area\"  # Class definitions: 0=background, 1=industrial_area\n",
    "    PALETTE = [0], [1]  # Grayscale palette | Background: black, Industrial area: white\n",
    "    ORIGINAL_PALETTE = [90, 90, 90], [10, 10, 10]  # Background: gray, Industrial area: black\n",
    "\n",
    "    DIRECTORIES = [\"images\", \"masks\", \"gems\", \"air\"]\n",
    "    DATA_LIST = [\n",
    "        SentinelIndex.TRAIN, SentinelIndex.VALID,\n",
    "        SentinelIndex.TRAIN_MASK, SentinelIndex.VALID_MASK,\n",
    "        SentinelIndex.TRAIN_GEMS, SentinelIndex.VALID_GEMS,\n",
    "        SentinelIndex.TRAIN_AIR, SentinelIndex.VALID_AIR,\n",
    "    ]\n",
    "    TRAIN_LIST = [SentinelIndex.TRAIN, SentinelIndex.TRAIN_MASK, SentinelIndex.TRAIN_GEMS, SentinelIndex.TRAIN_AIR]  # should be matched order with extract_dirs and valid_list\n",
    "    VALID_LIST = [SentinelIndex.VALID, SentinelIndex.VALID_MASK, SentinelIndex.VALID_GEMS, SentinelIndex.VALID_AIR]\n",
    "\n",
    "    @classmethod\n",
    "    async def download_method(cls, url, root, filename):\n",
    "        loop = asyncio.get_event_loop()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            await loop.run_in_executor(executor, utils.download_url, url, root, filename)\n",
    "\n",
    "    @classmethod\n",
    "    async def extract_method(cls, from_path, to_path):\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                await loop.run_in_executor(executor, utils.extract_archive, from_path, to_path)\n",
    "        except FileExistsError as e:\n",
    "            traceback.print_exc()\n",
    "            raise FileExistsError(str(e) + \"\\nPlease use Python 3.13 or later. 3.12 or earlier versions not support unzip over existing directory.\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path] = None,\n",
    "        train: bool = True,\n",
    "        data_type: DatasetModals | list[DatasetModals] = DatasetModals.RGB,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sentinel-2 dataset for semantic segmentation.\n",
    "        \n",
    "        Args:\n",
    "            root: Dataset root directory\n",
    "            train: True for training set, False for validation set\n",
    "            transforms: Joint transforms for image+mask\n",
    "            transform: Image transforms\n",
    "            target_transform: Mask transforms\n",
    "        \"\"\"\n",
    "        super().__init__(root, transforms=transforms, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(self.download(root))\n",
    "\n",
    "        self.types = data_type if isinstance(data_type, list) else [data_type]\n",
    "        self.root = path.join(root, self.dataset_name)\n",
    "        self.train = train\n",
    "        split = \"train\" if train else \"val\"\n",
    "        self.images, self.masks, self.gems, self.air = lists = [], [], [], []\n",
    "        extract_dirs = [path.join(self.root, anno, split) for anno in self.DIRECTORIES]\n",
    "        for lst, anno in zip(lists, self.DIRECTORIES):\n",
    "            lst.extend(sorted(glob(path.join(extract_dirs[self.DIRECTORIES.index(anno)], \"*.tif\"))))\n",
    "\n",
    "        assert len(self.images) == len(self.masks), \\\n",
    "            f\"Number of images ({len(self.images)}) and masks ({len(self.masks)}) do not match.\"\n",
    "        if DatasetModals.GEMS in self.types:\n",
    "            assert len(self.images) == len(self.gems), \\\n",
    "                f\"Number of images ({len(self.images)}) and GEMS data ({len(self.gems)}) do not match.\"\n",
    "        if DatasetModals.AIR in self.types:\n",
    "            assert len(self.images) == len(self.air), \\\n",
    "                f\"Number of images ({len(self.images)}) and AIR data ({len(self.air)}) do not match.\"\n",
    "\n",
    "        self.cached_data = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cached_data:\n",
    "            image, mask, gems, air = self.cached_data[idx]\n",
    "        else:\n",
    "            # Load image/mask using default_loader\n",
    "            image = self.load_raster(self.images[idx], channels=(1, 2, 3, 4) if DatasetModals.NIR in self.types else (1, 2, 3), normalize=True)\n",
    "            mask = self.load_raster(self.masks[idx])\n",
    "\n",
    "            # Convert mask to grayscale\n",
    "            mask = torch.where(mask == 10, 1, 0).to(torch.uint8)\n",
    "\n",
    "            # Load additional data if specified\n",
    "            gems, air = None, None\n",
    "            if DatasetModals.GEMS in self.types:\n",
    "                gems = self.load_raster(self.gems[idx], channels=range(1, 11), normalize=True)\n",
    "            if DatasetModals.AIR in self.types:\n",
    "                air = self.load_raster(self.air[idx], channels=range(1, 7), normalize=True)\n",
    "    \n",
    "            # Cache the loaded data\n",
    "            self.cached_data[idx] = (image, mask, gems, air)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            # Joint transforms (e.g., albumentations)\n",
    "            try:\n",
    "                image, mask, gems, air = self.transforms(image, mask, gems, air)\n",
    "            except Exception:\n",
    "                image, mask = self.transforms(image, mask)\n",
    "        else:\n",
    "            # Individual transforms\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            if self.target_transform:\n",
    "                try:\n",
    "                    mask, gems, air = self.target_transform(mask, gems, air)\n",
    "                except Exception:\n",
    "                    mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask, gems, air\n",
    "\n",
    "    def load_raster(self, path: Path, channels=(1,), normalize=False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Load TIF image using rasterio.\n",
    "    \n",
    "        Args:\n",
    "            path: Path to TIF file\n",
    "            channels: Channels to read (1-based indexing)\n",
    "            normalize: Whether to normalize the image to 0-1 range\n",
    "    \n",
    "        Returns:\n",
    "            Normalized image array in (C, H, W) format\n",
    "        \"\"\"\n",
    "        with rasterio.open(path) as src:\n",
    "            data = src.read(channels)\n",
    "\n",
    "        # Normalize to 0-1\n",
    "        if normalize:\n",
    "            data = data.astype(np.float32)\n",
    "            # Channel-wise Z-score normalization\n",
    "            for i in range(data.shape[0]):\n",
    "                channel_min = data[i].min()\n",
    "                channel_max = data[i].max()\n",
    "                diff = channel_max - channel_min\n",
    "                if diff != 0:\n",
    "                    data[i] = (data[i] - channel_min) / diff\n",
    "                else:\n",
    "                    data[i] = 0\n",
    "        return torch.from_numpy(data)\n",
    "\n",
    "    @classmethod\n",
    "    async def download(cls, root: str):\n",
    "        dataset_root = path.join(root, cls.dataset_name)\n",
    "        if path.exists(dataset_root):  # If the dataset directory already exists, skip download\n",
    "            return\n",
    "\n",
    "        print(f\"INFO: Downloading '{cls.dataset_name}' from server to {root}...\")\n",
    "        routines = []\n",
    "        for data in cls.DATA_LIST:\n",
    "            if path.isfile(path.join(root, data.value)):\n",
    "                print(f\"INFO: Dataset archive {data.value} found in the root directory. Skipping download.\")\n",
    "                continue\n",
    "\n",
    "            routines.extend(cls.download_method(url, root=root, filename=file) for url, file in zip(data.urls, data.names))\n",
    "        await tqdm.gather(*routines, desc=f\"Downloading {len(routines)} files\")\n",
    "\n",
    "        print(f\"INFO: Extracting '{cls.dataset_name}' dataset...\")\n",
    "        routines = []\n",
    "        as_train, as_valid = lambda d: path.join(d, \"train\"), lambda d: path.join(d, \"val\")\n",
    "        extract_dirs = [path.join(dataset_root, anno) for anno in cls.DIRECTORIES]\n",
    "        for trains, dirs in zip(cls.TRAIN_LIST, extract_dirs):\n",
    "            routines.extend(cls.extract_method(path.join(root, file), to_path=as_train(dirs)) for file in trains.names)\n",
    "        for valids, dirs in zip(cls.VALID_LIST, extract_dirs):\n",
    "            routines.extend(cls.extract_method(path.join(root, file), to_path=as_valid(dirs)) for file in valids.names)\n",
    "\n",
    "        await tqdm.gather(*routines, desc=f\"Extracting {len(routines)} files\")\n",
    "\n",
    "\n",
    "class SentinelDatasetForSegmentation(SentinelDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea74794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class LandsatDataset(SentinelDataset):\n",
    "    dataset_name = \"Landsat\"\n",
    "\n",
    "    CLASSES = \"background\", \"urban_area\"  # Class definitions: 0=background, 1=urban_area\n",
    "    PALETTE = [0], [1]  # Grayscale palette | Background: black, Urban area: white\n",
    "    ORIGINAL_PALETTE = [90, 90, 90], [10, 10, 10]  # Background: gray, Urban area: black\n",
    "\n",
    "    DIRECTORIES = [\"images\", \"masks\"]\n",
    "    DATA_LIST = [\n",
    "        LandsatIndex.TRAIN, LandsatIndex.VALID,\n",
    "        LandsatIndex.TRAIN_MASK, LandsatIndex.VALID_MASK\n",
    "    ]\n",
    "    TRAIN_LIST = [LandsatIndex.TRAIN, LandsatIndex.TRAIN_MASK]  # should be matched order with extract_dirs and valid_list\n",
    "    VALID_LIST = [LandsatIndex.VALID, LandsatIndex.VALID_MASK]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path] = None,\n",
    "        train: bool = True,\n",
    "        data_type: DatasetModals | list[DatasetModals] = DatasetModals.RGB,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        types = [dt for dt in data_type if dt in (DatasetModals.RGB, DatasetModals.NIR)]\n",
    "        super().__init__(root, train, types, transforms, transform, target_transform)\n",
    "\n",
    "\n",
    "class LandsatDatasetForSegmentation(LandsatDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a0919",
   "metadata": {},
   "source": [
    "#### 3.2.3. 데이터셋 인스턴스 생성\n",
    "\n",
    "- Train, Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958226d",
   "metadata": {},
   "source": [
    "##### 3.2.3.1. 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973243d0d0632803",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# 모달리티 설정\n",
    "modals = [DatasetModals.RGB, DatasetModals.NIR]\n",
    "\n",
    "sentinels = DatasetHolder(\n",
    "    train=SentinelDatasetForSegmentation(root=DATA_ROOT, train=True, data_type=modals),\n",
    "    valid=SentinelDatasetForSegmentation(root=DATA_ROOT, train=False, data_type=modals)\n",
    ")\n",
    "sentinels.test = sentinels.valid  # test set은 valid set과 동일\n",
    "\n",
    "landsats = DatasetHolder(\n",
    "    train=LandsatDatasetForSegmentation(root=DATA_ROOT, train=True, data_type=modals),\n",
    "    valid=LandsatDatasetForSegmentation(root=DATA_ROOT, train=False, data_type=modals)\n",
    ")\n",
    "landsats.test = landsats.valid  # test set은 valid set과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4368a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 확인\n",
    "sentinels.train.images[0], sentinels.train.masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1973a6e3a7a4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 검증\n",
    "sentinels.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11feab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinels.train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2dc6e7",
   "metadata": {},
   "source": [
    "##### 3.2.3.2 출력 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d102cd1b5fdcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [0, 100, 1000]:\n",
    "    rgb_image, mask_image, _, _ = sentinels.train[idx]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(rgb_image.permute(1, 2, 0))\n",
    "    axes[0].set_title('Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(mask_image.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(\"Mask\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(axes[1].imshow(mask_image.squeeze(), cmap='gray'), ax=axes[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abedc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB 채널만 시각화\n",
    "for idx in [0, 100, 1000]:\n",
    "    rgb_image, mask_image, _, _ = sentinels.train[idx]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(rgb_image[:3, :, :].permute(1, 2, 0))\n",
    "    axes[0].set_title('Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(mask_image.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(\"Mask\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(axes[1].imshow(mask_image.squeeze(), cmap='gray'), ax=axes[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDVI 계산\n",
    "rgb_image, _, _, _ = sentinels.train[1000]\n",
    "nir, red = rgb_image[3, :, :], rgb_image[0, :, :]  # NIR 채널과 Red 채널 추출\n",
    "denominator = nir + red\n",
    "ndvi = torch.where(denominator > 0, (nir - red) / denominator, 0)\n",
    "\n",
    "print(\"NDVI 계산 완료!\")\n",
    "print(f\"NDVI 값 범위: {ndvi.min():.3f} ~ {ndvi.max():.3f}\")\n",
    "\n",
    "# NDVI 시각화\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(ndvi, cmap='RdYlGn')  # 빨강=낮음(식생 없음), 초록=높음(식생 풍부)\n",
    "plt.colorbar(label=\"NDVI\")\n",
    "plt.title(\"NDVI Map\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752cebd36e5c809",
   "metadata": {},
   "source": [
    "## 4. DataLoader\n",
    "\n",
    "- 데이터 로더 생성\n",
    "- A100 기준 배치 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d124eb51bc123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "PRETRAIN_BATCH_SIZE = 128, 500\n",
    "BATCH_SIZE = 64, 128, 128\n",
    "#PRETRAIN_BATCH_SIZE = 2, 8, 8\n",
    "#BATCH_SIZE = 2, 8, 8\n",
    "\n",
    "# Workers\n",
    "WORKERS = 32\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7255ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, masks, gems, air = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    masks = torch.stack(masks)\n",
    "    gems = torch.stack(gems) if gems[0] is not None else None\n",
    "    air = torch.stack(air) if air[0] is not None else None\n",
    "    return images, masks, gems, air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_loaders = DataLoaderHolder(\n",
    "    train=DataLoader(sentinels.train, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=WORKERS, collate_fn=collate_fn),\n",
    "    valid=DataLoader(sentinels.valid, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=WORKERS, collate_fn=collate_fn),\n",
    ")\n",
    "sentinel_loaders.test = sentinel_loaders.valid\n",
    "landsat_loaders = DataLoaderHolder(\n",
    "    train=DataLoader(landsats.train, batch_size=PRETRAIN_BATCH_SIZE[0], shuffle=True, num_workers=WORKERS, collate_fn=collate_fn),\n",
    "    valid=DataLoader(landsats.valid, batch_size=PRETRAIN_BATCH_SIZE[1], shuffle=False, num_workers=WORKERS, collate_fn=collate_fn)\n",
    ")\n",
    "landsat_loaders.test = landsat_loaders.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask(image, mask, alpha=0.4, color=(1,0,0)):\n",
    "    \"\"\"\n",
    "    image: (H,W,3) RGB\n",
    "    mask:  (H,W) binary mask (0 or 1)\n",
    "    alpha: 투명도\n",
    "    color: 오버레이 색상 (R,G,B)\n",
    "    \"\"\"\n",
    "    overlay = image.numpy().copy()\n",
    "    overlay[mask > 0.5] = (1-alpha)*overlay[mask > 0.5] + alpha*np.array(color)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "def visualize_overlay(X, Y, num_samples=3, set_name=\"Train\"):\n",
    "    idxs = random.sample(range(len(X)), num_samples)\n",
    "    plt.figure(figsize=(12, num_samples*4))\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        img = X[idx]\n",
    "        label = Y[idx]\n",
    "\n",
    "        # 원본\n",
    "        plt.subplot(num_samples, 3, i*3+1)\n",
    "        plt.imshow(img[:3, :, :].permute(1, 2, 0))\n",
    "        plt.title(f\"{set_name} Image {idx}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # 마스크\n",
    "        plt.subplot(num_samples, 3, i*3+2)\n",
    "        plt.imshow(label.squeeze(), cmap=\"gray\")\n",
    "        plt.title(\"Foreground Mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # 오버레이\n",
    "        overlayed = overlay_mask(img[:3, :, :].permute(1, 2, 0), label.squeeze())\n",
    "        plt.subplot(num_samples, 3, i*3+3)\n",
    "        plt.imshow(overlayed)\n",
    "        plt.title(\"Overlay\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 배치 데이터 시각화\n",
    "train_sample = next(iter(sentinel_loaders.train))\n",
    "valid_sample = next(iter(sentinel_loaders.valid))\n",
    "visualize_overlay(train_sample[0], train_sample[1], num_samples=2, set_name=\"Train\")\n",
    "visualize_overlay(valid_sample[0], valid_sample[1], num_samples=2, set_name=\"Valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4053c037f244da",
   "metadata": {},
   "source": [
    "## 5. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c344106",
   "metadata": {},
   "source": [
    "- 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "notebook_progress_callback",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookProgressCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_progress_bar = None\n",
    "        self.val_progress_bar = None\n",
    "        self.best_val_miou = 0.0\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        total_batches = len(trainer.train_dataloader)\n",
    "        self.train_progress_bar = NotebookProgressBar(\n",
    "            total_batches, prefix=f\"Training {trainer.current_epoch + 1}\",\n",
    "        )\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        metrics = trainer.callback_metrics\n",
    "        loss = metrics.get('train_loss_step', 0)\n",
    "        miou = metrics.get('train_miou_step', 0)\n",
    "        current_lr = trainer.optimizers[0].param_groups[0]['lr']\n",
    "        self.train_progress_bar.update(batch_idx+1, comment=f\"Loss={loss:.4f}, mIoU={miou:.4f}, LR={current_lr:.2e}\")\n",
    "\n",
    "    def on_validation_epoch_start(self, trainer, pl_module):\n",
    "        total_batches = len(trainer.val_dataloaders)\n",
    "        self.val_progress_bar = NotebookProgressBar(\n",
    "            total_batches, prefix=f\"Validating {trainer.current_epoch + 1}\"\n",
    "        )\n",
    "\n",
    "    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        total_batches = len(trainer.val_dataloaders)\n",
    "        if batch_idx < total_batches-1:\n",
    "            self.val_progress_bar.update(batch_idx+1, comment=\"\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics\n",
    "        val_loss = metrics.get('val_loss', 0)\n",
    "        val_miou = metrics.get('val_miou', 0)\n",
    "        self.val_progress_bar.update(self.val_progress_bar.total, comment=f\"Loss={val_loss:.4f}, mIoU={val_miou:.4f}\")\n",
    "        if val_miou > self.best_val_miou:\n",
    "            self.best_val_miou = val_miou\n",
    "            print(f\"Best mIoU so far: {self.best_val_miou:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2D -> BatchNorm -> ReLU) * 2\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet architecture matching the TensorFlow model\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: 64 -> 128 -> 256 -> 512 -> 1024 (bottleneck)\n",
    "    - Decoder: 512 -> 256 -> 128 -> 64\n",
    "    - Output: 2 classes (background, foreground)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.dec1 = DoubleConv(1024 + 512, 512)\n",
    "        \n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.dec2 = DoubleConv(512 + 256, 256)\n",
    "        \n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.dec3 = DoubleConv(256 + 128, 128)\n",
    "        \n",
    "        self.up4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.dec4 = DoubleConv(128 + 64, 64)\n",
    "        \n",
    "        # Output\n",
    "        self.out_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)      # 64 channels\n",
    "        x = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.enc2(x)      # 128 channels\n",
    "        x = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.enc3(x)      # 256 channels\n",
    "        x = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.enc4(x)      # 512 channels\n",
    "        x = self.pool4(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)   # 1024 channels\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, enc4], dim=1)\n",
    "        x = self.dec1(x)         # 512 channels\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, enc3], dim=1)\n",
    "        x = self.dec2(x)         # 256 channels\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, enc2], dim=1)\n",
    "        x = self.dec3(x)         # 128 channels\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, enc1], dim=1)\n",
    "        x = self.dec4(x)         # 64 channels\n",
    "        \n",
    "        # Output\n",
    "        x = self.out_conv(x)     # num_classes channels\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # 모델 정의\n",
    "        self.model = UNet(in_channels=3, num_classes=2)  # 2 classes\n",
    "\n",
    "        # 손실 함수: CrossEntropyLoss (2채널 출력용)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # mIoU 계산을 위한 변수\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def calculate_miou(self, preds, masks, num_classes=2):\n",
    "        \"\"\"mIoU 계산 함수 (multi-class)\"\"\"\n",
    "        # preds: (B, 2, H, W) - logits\n",
    "        # masks: (B, H, W) - class indices (0 or 1)\n",
    "        \n",
    "        # Get predicted class\n",
    "        preds = torch.argmax(preds, dim=1)  # (B, H, W)\n",
    "        \n",
    "        ious = []\n",
    "        for cls in range(num_classes):\n",
    "            pred_cls = (preds == cls)\n",
    "            mask_cls = (masks == cls)\n",
    "            \n",
    "            intersection = (pred_cls & mask_cls).sum().float()\n",
    "            union = (pred_cls | mask_cls).sum().float()\n",
    "            \n",
    "            if union == 0:\n",
    "                iou = 1.0 if intersection == 0 else 0.0\n",
    "            else:\n",
    "                iou = intersection / union\n",
    "            \n",
    "            ious.append(iou)\n",
    "        \n",
    "        return torch.stack(ious).mean()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks, _, _ = batch\n",
    "        # images: (B, 3, H, W)\n",
    "        # masks: (B, H, W) - 0 or 1 (class indices)\n",
    "        \n",
    "        outputs = self(images)  # (B, 2, H, W)\n",
    "        \n",
    "        # CrossEntropyLoss는 (B, C, H, W)와 (B, H, W) 형태를 받음\n",
    "        loss = self.criterion(outputs, masks.long())\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # mIoU 계산\n",
    "        miou = self.calculate_miou(outputs, masks)\n",
    "\n",
    "        # 로깅\n",
    "        self.log('train_loss', loss, prog_bar=False, on_step=False, on_epoch=True)\n",
    "        self.log('train_loss_step', loss, prog_bar=False, on_step=True, on_epoch=False, batch_size=batch_size)\n",
    "        self.log('train_miou', miou, prog_bar=False, on_step=False, on_epoch=True)\n",
    "        self.log('train_miou_step', miou, prog_bar=False, on_step=True, on_epoch=False, batch_size=batch_size)\n",
    "\n",
    "        # WandB 로깅\n",
    "        wandb.log({\n",
    "            'train_loss': loss.item(), \n",
    "            'train_miou': miou.item(),\n",
    "            'step': self.global_step\n",
    "        })\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks, _, _ = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, masks.long())\n",
    "\n",
    "        # mIoU 계산\n",
    "        miou = self.calculate_miou(outputs, masks)\n",
    "\n",
    "        # 결과 저장\n",
    "        self.validation_step_outputs.append({\n",
    "            'val_loss': loss,\n",
    "            'val_miou': miou\n",
    "        })\n",
    "\n",
    "        return {'val_loss': loss, 'val_miou': miou}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # 에폭별 평균 계산\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in self.validation_step_outputs]).mean()\n",
    "        avg_miou = torch.stack([x['val_miou'] for x in self.validation_step_outputs]).mean()\n",
    "\n",
    "        # 로깅\n",
    "        self.log('val_loss', avg_loss, prog_bar=False)\n",
    "        self.log('val_miou', avg_miou, prog_bar=False)\n",
    "\n",
    "        # WandB 로깅\n",
    "        wandb.log({\n",
    "            'val_loss': avg_loss.item(),\n",
    "            'val_miou': avg_miou.item(),\n",
    "            'epoch': self.current_epoch\n",
    "        })\n",
    "\n",
    "        # 초기화\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, masks, _, _ = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, masks.long())\n",
    "\n",
    "        # mIoU 계산\n",
    "        miou = self.calculate_miou(outputs, masks)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_miou', miou)\n",
    "\n",
    "        return {'test_loss': loss, 'test_miou': miou}\n",
    "\n",
    "    def reset_optimizers_cache(self):\n",
    "        if hasattr(self, '_optim_dict_cache'):\n",
    "            del self._optim_dict_cache\n",
    "\n",
    "    def configure_optimizers(self, learning_rate=None, max_epochs=None):\n",
    "        if hasattr(self, '_optim_dict_cache'):\n",
    "            return self._optim_dict_cache\n",
    "        else:\n",
    "            self.hparams.learning_rate = learning_rate if learning_rate is not None else 1e-4\n",
    "            optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "            # Cosine Annealing Scheduler\n",
    "            self.hparams.max_epochs = max_epochs if max_epochs is not None else 50\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=self.hparams.max_epochs,\n",
    "                eta_min=1e-6\n",
    "            )\n",
    "\n",
    "            self._optim_dict_cache = {\n",
    "                'optimizer': optimizer,\n",
    "                'lr_scheduler': {\n",
    "                    'scheduler': scheduler,\n",
    "                    'interval': 'epoch',\n",
    "                    'frequency': 1\n",
    "                }\n",
    "            }\n",
    "\n",
    "            return self._optim_dict_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44373f47d600fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "set_seed(2025)\n",
    "model = SegmentationModel()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374c7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mission 3 모델 가중치 불러오기\n",
    "model.model.load_state_dict(torch.hub(\"https://github.com/b-re-w/K-ICT_DataCreatorCamp_2025/releases/download/mission3/mission_3.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfdb161ae65bd5",
   "metadata": {},
   "source": [
    "## 6. Train\n",
    "\n",
    "- Epoch 기반 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea451a",
   "metadata": {},
   "source": [
    "### 6.1. Landsat 데이터셋으로 역전이학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48316cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "set_seed(2025)\n",
    "model.configure_optimizers(learning_rate=LEARNING_RATE, max_epochs=EPOCHS)\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=1, enable_progress_bar=False, callbacks=[NotebookProgressCallback()], enable_model_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5273c531e2c1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=landsat_loaders.train,\n",
    "    val_dataloaders=landsat_loaders.valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff911ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간 결과 저장\n",
    "makedirs(\"./results/\", exist_ok=True)\n",
    "save_path = f\"./results/mission_4__reverse.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"INFO: Final model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"results/mission_4__reverse.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc74830",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsats.train.cached_data = {}\n",
    "landsats.valid.cached_data = {}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b88684",
   "metadata": {},
   "source": [
    "### 6.2. Sentinel 데이터셋으로 최종 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_optimizers_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19378ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "set_seed(2025)\n",
    "model.configure_optimizers(learning_rate=LEARNING_RATE, max_epochs=EPOCHS)\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=1, enable_progress_bar=False, callbacks=[NotebookProgressCallback()], enable_model_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf296fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=sentinel_loaders.train,\n",
    "    val_dataloaders=sentinel_loaders.valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 저장\n",
    "makedirs(\"./results/\", exist_ok=True)\n",
    "save_path = f\"./results/mission_4__transfer.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"INFO: Final model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0aace760f618d0",
   "metadata": {},
   "source": [
    "## 7. Evaluate\n",
    "\n",
    "- 최종 성능 확인 및 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5655e026216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Evaluation ===\")\n",
    "trainer.test(model, dataloaders=sentinel_loaders.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0e803b70cf08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(\"./results/\", exist_ok=True)\n",
    "save_path = f\"./results/mission_4.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"INFO: Final model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508ab0fd10e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정상 저장 확인\n",
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59c9498f5db854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creator-camp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
