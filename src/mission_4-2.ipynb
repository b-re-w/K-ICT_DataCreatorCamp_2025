{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044d9c6a",
   "metadata": {},
   "source": [
    "# 2025 데이터 크리에이터 캠프\n",
    "\n",
    "@PHASE: Mission 4-2\n",
    "\n",
    "@TEAM: 최후의 인공지능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5ea4cfae13401",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability\n",
    "\n",
    "- GPU 번호 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e888eba8b18b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa704fa81e6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066676dc",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "- 의존성 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.utils.notebook import NotebookProgressBar\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fe787",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()  # 시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215d175116b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"Mission_4-2\"\n",
    "RUN_NAME = \"UNet\"\n",
    "\n",
    "# WandB Initialization\n",
    "wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e1c723f76fc4f",
   "metadata": {},
   "source": [
    "## 3. Define Dataset\n",
    "\n",
    "- 데이터셋 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b4212f",
   "metadata": {},
   "source": [
    "### 3.1. 데이터셋 홀더 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetHolder:\n",
    "    train: Dataset = None\n",
    "    valid: Dataset = None\n",
    "    test: Dataset = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        print(f\"INFO: Dataset loaded successfully. Number of samples - \", end='')\n",
    "        if self.train:\n",
    "            print(f\"Train: {len(self.train)}\", end='')\n",
    "        if self.valid:\n",
    "            if self.train: print(', ', end='')\n",
    "            print(f\"Valid: {len(self.valid)}\", end='')\n",
    "        if self.test:\n",
    "            if self.train: print(', ', end='')\n",
    "            print(f\"Test: {len(self.test)}\", end='')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataLoaderHolder:\n",
    "    train: object = None\n",
    "    valid: object = None\n",
    "    test: object = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7375653",
   "metadata": {},
   "source": [
    "### 3.2. 데이터셋 클래스 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d49f00",
   "metadata": {},
   "source": [
    "#### 3.2.1. 데이터셋 인덱스 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e51d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "# Github Release URL for datasets\n",
    "# This will be removed after the contest ends due to the copyright issue.\n",
    "base_git_path = \"https://github.com/b-re-w/K-ICT_DataCreatorCamp_2025/releases/download/dt/\"\n",
    "\n",
    "\n",
    "class KompsatIndex(Enum):\n",
    "    TRAIN = \"TS_KS.zip\"\n",
    "    VALID = \"VS_KS.zip\"\n",
    "    TRAIN_BBOX = \"TL_KS_BBOX.zip\"\n",
    "    VALID_BBOX = \"VL_KS_BBOX.zip\"\n",
    "    TRAIN_LINE = \"TL_KS_LINE.zip\"\n",
    "    VALID_LINE = \"VL_KS_LINE.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\"\n",
    "\n",
    "\n",
    "class SentinelIndex(Enum):\n",
    "    TRAIN = \"TS_SN10_SN10.zip\"\n",
    "    VALID = \"VS_SN10_SN10.zip\"\n",
    "    TRAIN_MASK = \"TL_SN10.zip\"\n",
    "    VALID_MASK = \"VL_SN10.zip\"\n",
    "    TRAIN_GEMS = \"TS_SN10_GEMS.zip\"\n",
    "    VALID_GEMS = \"VS_SN10_GEMS.zip\"\n",
    "    TRAIN_AIR = \"TS_SN10_AIR_POLLUTION.zip\"\n",
    "    VALID_AIR = \"VS_SN10_AIR_POLLUTION.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\"\n",
    "\n",
    "    @property\n",
    "    def urls(self):\n",
    "        data_range = None\n",
    "        match self:\n",
    "            case SentinelIndex.TRAIN:\n",
    "                data_range = range(1, 9)\n",
    "            case SentinelIndex.TRAIN_GEMS:\n",
    "                data_range = range(1, 3)\n",
    "            case _:\n",
    "                return [self.url]\n",
    "        return [\n",
    "            self.url.replace(\".zip\", f\"_p{i}.zip\") for i in data_range\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def names(self):\n",
    "        data_range = None\n",
    "        match self:\n",
    "            case SentinelIndex.TRAIN:\n",
    "                data_range = range(1, 9)\n",
    "            case SentinelIndex.TRAIN_GEMS:\n",
    "                data_range = range(1, 3)\n",
    "            case _:\n",
    "                return [self.value]\n",
    "        return [\n",
    "            self.value.replace(\".zip\", f\"_p{i}.zip\") for i in data_range\n",
    "        ]\n",
    "\n",
    "\n",
    "class LandsatIndex(Enum):\n",
    "    TRAIN = \"TS_LS30_LS30.zip\"\n",
    "    VALID = \"VS_LS30_LS30.zip\"\n",
    "    TRAIN_MASK = \"TL_LS30.zip\"\n",
    "    VALID_MASK = \"VL_LS30.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\"\n",
    "\n",
    "    @property\n",
    "    def urls(self):\n",
    "        return [self.url]\n",
    "\n",
    "    @property\n",
    "    def names(self):\n",
    "        return [self.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020d1d7",
   "metadata": {},
   "source": [
    "#### 3.2.2. 목적별 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from os import path\n",
    "from glob import glob\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import VisionDataset, utils\n",
    "import rasterio\n",
    "\n",
    "from tqdm.asyncio import tqdm\n",
    "import concurrent.futures\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class DatasetModals(Enum):\n",
    "    RGB = \"rgb\"\n",
    "    NIR = \"nir\"\n",
    "    GEMS = \"gems\"\n",
    "    AIR = \"air\"\n",
    "\n",
    "\n",
    "class SentinelDataset(VisionDataset):\n",
    "    dataset_name = \"Sentinel\"\n",
    "\n",
    "    CLASSES = \"background\", \"industrial_area\"  # Class definitions: 0=background, 1=industrial_area\n",
    "    PALETTE = [0], [1]  # Grayscale palette | Background: black, Industrial area: white\n",
    "    ORIGINAL_PALETTE = [90, 90, 90], [10, 10, 10]  # Background: gray, Industrial area: black\n",
    "\n",
    "    DIRECTORIES = [\"images\", \"masks\", \"gems\", \"air\"]\n",
    "    DATA_LIST = [\n",
    "        SentinelIndex.TRAIN, SentinelIndex.VALID,\n",
    "        SentinelIndex.TRAIN_MASK, SentinelIndex.VALID_MASK,\n",
    "        SentinelIndex.TRAIN_GEMS, SentinelIndex.VALID_GEMS,\n",
    "        SentinelIndex.TRAIN_AIR, SentinelIndex.VALID_AIR,\n",
    "    ]\n",
    "    TRAIN_LIST = [SentinelIndex.TRAIN, SentinelIndex.TRAIN_MASK, SentinelIndex.TRAIN_GEMS, SentinelIndex.TRAIN_AIR]  # should be matched order with extract_dirs and valid_list\n",
    "    VALID_LIST = [SentinelIndex.VALID, SentinelIndex.VALID_MASK, SentinelIndex.VALID_GEMS, SentinelIndex.VALID_AIR]\n",
    "\n",
    "    @classmethod\n",
    "    async def download_method(cls, url, root, filename):\n",
    "        loop = asyncio.get_event_loop()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            await loop.run_in_executor(executor, utils.download_url, url, root, filename)\n",
    "\n",
    "    @classmethod\n",
    "    async def extract_method(cls, from_path, to_path):\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                await loop.run_in_executor(executor, utils.extract_archive, from_path, to_path)\n",
    "        except FileExistsError as e:\n",
    "            traceback.print_exc()\n",
    "            raise FileExistsError(str(e) + \"\\nPlease use Python 3.13 or later. 3.12 or earlier versions not support unzip over existing directory.\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path] = None,\n",
    "        train: bool = True,\n",
    "        data_type: DatasetModals | list[DatasetModals] = DatasetModals.RGB,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sentinel-2 dataset for semantic segmentation.\n",
    "        \n",
    "        Args:\n",
    "            root: Dataset root directory\n",
    "            train: True for training set, False for validation set\n",
    "            transforms: Joint transforms for image+mask\n",
    "            transform: Image transforms\n",
    "            target_transform: Mask transforms\n",
    "        \"\"\"\n",
    "        super().__init__(root, transforms=transforms, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(self.download(root))\n",
    "\n",
    "        self.types = data_type if isinstance(data_type, list) else [data_type]\n",
    "        self.root = path.join(root, self.dataset_name)\n",
    "        self.train = train\n",
    "        split = \"train\" if train else \"val\"\n",
    "        self.images, self.masks, self.gems, self.air = lists = [], [], [], []\n",
    "        extract_dirs = [path.join(self.root, anno, split) for anno in self.DIRECTORIES]\n",
    "        for lst, anno in zip(lists, self.DIRECTORIES):\n",
    "            lst.extend(sorted(glob(path.join(extract_dirs[self.DIRECTORIES.index(anno)], \"*.tif\"))))\n",
    "\n",
    "        assert len(self.images) == len(self.masks), \\\n",
    "            f\"Number of images ({len(self.images)}) and masks ({len(self.masks)}) do not match.\"\n",
    "        if DatasetModals.GEMS in self.types:\n",
    "            assert len(self.images) == len(self.gems), \\\n",
    "                f\"Number of images ({len(self.images)}) and GEMS data ({len(self.gems)}) do not match.\"\n",
    "        if DatasetModals.AIR in self.types:\n",
    "            assert len(self.images) == len(self.air), \\\n",
    "                f\"Number of images ({len(self.images)}) and AIR data ({len(self.air)}) do not match.\"\n",
    "\n",
    "        self.cached_data = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cached_data:\n",
    "            image, mask, gems, air = self.cached_data[idx]\n",
    "        else:\n",
    "            # Load image/mask using default_loader\n",
    "            image = self.load_raster(self.images[idx], channels=(1, 2, 3, 4) if DatasetModals.NIR in self.types else (1, 2, 3), normalize=True)\n",
    "            mask = self.load_raster(self.masks[idx])\n",
    "\n",
    "            # Convert mask to grayscale\n",
    "            mask = torch.where(mask == 10, 1, 0).to(torch.uint8)\n",
    "\n",
    "            # Load additional data if specified\n",
    "            gems, air = None, None\n",
    "            if DatasetModals.GEMS in self.types:\n",
    "                gems = self.load_raster(self.gems[idx], channels=range(1, 11), normalize=True)\n",
    "            if DatasetModals.AIR in self.types:\n",
    "                air = self.load_raster(self.air[idx], channels=range(1, 7), normalize=True)\n",
    "    \n",
    "            # Cache the loaded data\n",
    "            self.cached_data[idx] = (image, mask, gems, air)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            # Joint transforms (e.g., albumentations)\n",
    "            try:\n",
    "                image, mask, gems, air = self.transforms(image, mask, gems, air)\n",
    "            except Exception:\n",
    "                image, mask = self.transforms(image, mask)\n",
    "        else:\n",
    "            # Individual transforms\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            if self.target_transform:\n",
    "                try:\n",
    "                    mask, gems, air = self.target_transform(mask, gems, air)\n",
    "                except Exception:\n",
    "                    mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask, gems, air\n",
    "\n",
    "    def load_raster(self, path: Path, channels=(1,), normalize=False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Load TIF image using rasterio.\n",
    "    \n",
    "        Args:\n",
    "            path: Path to TIF file\n",
    "            channels: Channels to read (1-based indexing)\n",
    "            normalize: Whether to normalize the image to 0-1 range\n",
    "    \n",
    "        Returns:\n",
    "            Normalized image array in (C, H, W) format\n",
    "        \"\"\"\n",
    "        with rasterio.open(path) as src:\n",
    "            data = src.read(channels)\n",
    "\n",
    "        # Normalize to 0-1\n",
    "        if normalize:\n",
    "            data = data.astype(np.float32)\n",
    "            # Channel-wise Z-score normalization\n",
    "            for i in range(data.shape[0]):\n",
    "                channel_min = data[i].min()\n",
    "                channel_max = data[i].max()\n",
    "                diff = channel_max - channel_min\n",
    "                if diff != 0:\n",
    "                    data[i] = (data[i] - channel_min) / diff\n",
    "                else:\n",
    "                    data[i] = 0\n",
    "        return torch.from_numpy(data)\n",
    "\n",
    "    @classmethod\n",
    "    async def download(cls, root: str):\n",
    "        dataset_root = path.join(root, cls.dataset_name)\n",
    "        if path.exists(dataset_root):  # If the dataset directory already exists, skip download\n",
    "            return\n",
    "\n",
    "        print(f\"INFO: Downloading '{cls.dataset_name}' from server to {root}...\")\n",
    "        routines = []\n",
    "        for data in cls.DATA_LIST:\n",
    "            if path.isfile(path.join(root, data.value)):\n",
    "                print(f\"INFO: Dataset archive {data.value} found in the root directory. Skipping download.\")\n",
    "                continue\n",
    "\n",
    "            routines.extend(cls.download_method(url, root=root, filename=file) for url, file in zip(data.urls, data.names))\n",
    "        await tqdm.gather(*routines, desc=f\"Downloading {len(routines)} files\")\n",
    "\n",
    "        print(f\"INFO: Extracting '{cls.dataset_name}' dataset...\")\n",
    "        routines = []\n",
    "        as_train, as_valid = lambda d: path.join(d, \"train\"), lambda d: path.join(d, \"val\")\n",
    "        extract_dirs = [path.join(dataset_root, anno) for anno in cls.DIRECTORIES]\n",
    "        for trains, dirs in zip(cls.TRAIN_LIST, extract_dirs):\n",
    "            routines.extend(cls.extract_method(path.join(root, file), to_path=as_train(dirs)) for file in trains.names)\n",
    "        for valids, dirs in zip(cls.VALID_LIST, extract_dirs):\n",
    "            routines.extend(cls.extract_method(path.join(root, file), to_path=as_valid(dirs)) for file in valids.names)\n",
    "\n",
    "        await tqdm.gather(*routines, desc=f\"Extracting {len(routines)} files\")\n",
    "\n",
    "\n",
    "class SentinelDatasetForSegmentation(SentinelDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea74794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Callable\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class LandsatDataset(SentinelDataset):\n",
    "    dataset_name = \"Landsat\"\n",
    "\n",
    "    CLASSES = \"background\", \"urban_area\"  # Class definitions: 0=background, 1=urban_area\n",
    "    PALETTE = [0], [1]  # Grayscale palette | Background: black, Urban area: white\n",
    "    ORIGINAL_PALETTE = [90, 90, 90], [10, 10, 10]  # Background: gray, Urban area: black\n",
    "\n",
    "    DIRECTORIES = [\"images\", \"masks\"]\n",
    "    DATA_LIST = [\n",
    "        LandsatIndex.TRAIN, LandsatIndex.VALID,\n",
    "        LandsatIndex.TRAIN_MASK, LandsatIndex.VALID_MASK\n",
    "    ]\n",
    "    TRAIN_LIST = [LandsatIndex.TRAIN, LandsatIndex.TRAIN_MASK]  # should be matched order with extract_dirs and valid_list\n",
    "    VALID_LIST = [LandsatIndex.VALID, LandsatIndex.VALID_MASK]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path] = None,\n",
    "        train: bool = True,\n",
    "        data_type: DatasetModals | list[DatasetModals] = DatasetModals.RGB,\n",
    "        transforms: Optional[Callable] = None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        types = [dt for dt in data_type if dt in (DatasetModals.RGB, DatasetModals.NIR)]\n",
    "        super().__init__(root, train, types, transforms, transform, target_transform)\n",
    "\n",
    "\n",
    "class LandsatDatasetForSegmentation(LandsatDataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a0919",
   "metadata": {},
   "source": [
    "#### 3.2.3. 데이터셋 인스턴스 생성\n",
    "\n",
    "- Train, Valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958226d",
   "metadata": {},
   "source": [
    "##### 3.2.3.1. 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973243d0d0632803",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# 모달리티 설정\n",
    "modals = [DatasetModals.RGB, DatasetModals.NIR]\n",
    "\n",
    "sentinels = DatasetHolder(\n",
    "    train=SentinelDatasetForSegmentation(root=DATA_ROOT, train=True, data_type=modals),\n",
    "    valid=SentinelDatasetForSegmentation(root=DATA_ROOT, train=False, data_type=modals)\n",
    ")\n",
    "sentinels.test = sentinels.valid  # test set은 valid set과 동일\n",
    "\n",
    "landsats = DatasetHolder(\n",
    "    train=LandsatDatasetForSegmentation(root=DATA_ROOT, train=True, data_type=modals),\n",
    "    valid=LandsatDatasetForSegmentation(root=DATA_ROOT, train=False, data_type=modals)\n",
    ")\n",
    "landsats.test = landsats.valid  # test set은 valid set과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4368a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 확인\n",
    "sentinels.train.images[0], sentinels.train.masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1973a6e3a7a4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 검증\n",
    "sentinels.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11feab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinels.train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2dc6e7",
   "metadata": {},
   "source": [
    "##### 3.2.3.2 출력 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d102cd1b5fdcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [0, 100, 1000]:\n",
    "    rgb_image, mask_image, _, _ = sentinels.train[idx]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(rgb_image.permute(1, 2, 0))\n",
    "    axes[0].set_title('Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(mask_image.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(\"Mask\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(axes[1].imshow(mask_image.squeeze(), cmap='gray'), ax=axes[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abedc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB 채널만 시각화\n",
    "for idx in [0, 100, 1000]:\n",
    "    rgb_image, mask_image, _, _ = sentinels.train[idx]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axes[0].imshow(rgb_image[:3, :, :].permute(1, 2, 0))\n",
    "    axes[0].set_title('Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(mask_image.squeeze(), cmap='gray')\n",
    "    axes[1].set_title(\"Mask\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(axes[1].imshow(mask_image.squeeze(), cmap='gray'), ax=axes[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDVI 계산\n",
    "rgb_image, _, _, _ = sentinels.train[1000]\n",
    "nir, red = rgb_image[3, :, :], rgb_image[0, :, :]  # NIR 채널과 Red 채널 추출\n",
    "denominator = nir + red\n",
    "ndvi = torch.where(denominator > 0, (nir - red) / denominator, 0)\n",
    "\n",
    "print(\"NDVI 계산 완료!\")\n",
    "print(f\"NDVI 값 범위: {ndvi.min():.3f} ~ {ndvi.max():.3f}\")\n",
    "\n",
    "# NDVI 시각화\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(ndvi, cmap='RdYlGn')  # 빨강=낮음(식생 없음), 초록=높음(식생 풍부)\n",
    "plt.colorbar(label=\"NDVI\")\n",
    "plt.title(\"NDVI Map\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752cebd36e5c809",
   "metadata": {},
   "source": [
    "## 4. DataLoader\n",
    "\n",
    "- 데이터 로더 생성\n",
    "- A100 기준 배치 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d124eb51bc123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 8  # 3070 Ti\n",
    "#BATCH_SIZE = 32, 64, 64  # A100\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7255ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, masks, gems, air = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    masks = torch.stack(masks)\n",
    "    gems = torch.stack(gems) if gems[0] is not None else None\n",
    "    air = torch.stack(air) if air[0] is not None else None\n",
    "    return images, masks, gems, air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_loaders = DataLoaderHolder(\n",
    "    train=DataLoader(sentinels.train, batch_size=BATCH_SIZE[0], shuffle=True, collate_fn=collate_fn),\n",
    "    valid=DataLoader(sentinels.valid, batch_size=BATCH_SIZE[1], shuffle=False, collate_fn=collate_fn),\n",
    ")\n",
    "sentinel_loaders.test = sentinel_loaders.valid\n",
    "landsat_loaders = DataLoaderHolder(\n",
    "    train=DataLoader(landsats.train, batch_size=BATCH_SIZE[0], shuffle=True, collate_fn=collate_fn),\n",
    "    valid=DataLoader(landsats.valid, batch_size=BATCH_SIZE[1], shuffle=False, collate_fn=collate_fn)\n",
    ")\n",
    "landsat_loaders.test = landsat_loaders.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask(image, mask, alpha=0.4, color=(1,0,0)):\n",
    "    \"\"\"\n",
    "    image: (H,W,3) RGB\n",
    "    mask:  (H,W) binary mask (0 or 1)\n",
    "    alpha: 투명도\n",
    "    color: 오버레이 색상 (R,G,B)\n",
    "    \"\"\"\n",
    "    overlay = image.numpy().copy()\n",
    "    overlay[mask > 0.5] = (1-alpha)*overlay[mask > 0.5] + alpha*np.array(color)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "def visualize_overlay(X, Y, num_samples=3, set_name=\"Train\"):\n",
    "    idxs = random.sample(range(len(X)), num_samples)\n",
    "    plt.figure(figsize=(12, num_samples*4))\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        img = X[idx]\n",
    "        label = Y[idx]\n",
    "\n",
    "        # 원본\n",
    "        plt.subplot(num_samples, 3, i*3+1)\n",
    "        plt.imshow(img[:3, :, :].permute(1, 2, 0))\n",
    "        plt.title(f\"{set_name} Image {idx}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # 마스크\n",
    "        plt.subplot(num_samples, 3, i*3+2)\n",
    "        plt.imshow(label.squeeze(), cmap=\"gray\")\n",
    "        plt.title(\"Foreground Mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # 오버레이\n",
    "        overlayed = overlay_mask(img[:3, :, :].permute(1, 2, 0), label.squeeze())\n",
    "        plt.subplot(num_samples, 3, i*3+3)\n",
    "        plt.imshow(overlayed)\n",
    "        plt.title(\"Overlay\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 배치 데이터 시각화\n",
    "train_sample = next(iter(sentinel_loaders.train))\n",
    "valid_sample = next(iter(sentinel_loaders.valid))\n",
    "visualize_overlay(train_sample[0], train_sample[1], num_samples=2, set_name=\"Train\")\n",
    "visualize_overlay(valid_sample[0], valid_sample[1], num_samples=2, set_name=\"Valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4053c037f244da",
   "metadata": {},
   "source": [
    "## 5. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c344106",
   "metadata": {},
   "source": [
    "- 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44373f47d600fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "set_seed(2025)\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\", encoder_weights=None,\n",
    "    in_channels=4, classes=1,\n",
    "    encoder_depth=5, decoder_channels=(512, 256, 128, 64, 64)\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfdb161ae65bd5",
   "metadata": {},
   "source": [
    "## 6. Train\n",
    "\n",
    "- Epoch 기반 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea451a",
   "metadata": {},
   "source": [
    "### 6.1. Landsat 데이터셋으로 프리트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48316cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4, 1e-5\n",
    "\n",
    "set_seed(2025)\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5273c531e2c1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=landsat_loaders.train,\n",
    "    val_dataloaders=landsat_loaders.valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff911ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간 결과 저장\n",
    "makedirs(\"./results/\", exist_ok=True)\n",
    "save_path = f\"./results/mission_4__temp1.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"INFO: Final model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"results/mission_2__temp2.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b88684",
   "metadata": {},
   "source": [
    "### 6.2. Sentinel 데이터셋으로 최종 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19378ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4, 1e-5\n",
    "\n",
    "set_seed(2025)\n",
    "trainer = pl.Trainer(max_epochs=EPOCHS, log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf296fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 저장\n",
    "makedirs(\"./results/\", exist_ok=True)\n",
    "save_path = f\"./results/mission_4__temp2.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"INFO: Final model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0aace760f618d0",
   "metadata": {},
   "source": [
    "## 7. Evaluate\n",
    "\n",
    "- 최종 성능 확인 및 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b420442c9a00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, data_loader: DataLoader, data_loader_len: int):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5655e026216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Evaluation ===\")\n",
    "test_loader_len = int(len(sentinels.test)/BATCH_SIZE[2]+0.99)\n",
    "test_miou = evaluate(model, sentinel_loaders.test, test_loader_len)\n",
    "print(f\"INFO: Test mIoU - {test_miou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0e803b70cf08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(\"./results/\", exist_ok=True)\n",
    "save_path = f\"./results/mission_4__miou_{test_miou}.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"INFO: Final model saved to\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508ab0fd10e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정상 저장 확인\n",
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59c9498f5db854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creator-camp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
