{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044d9c6a",
   "metadata": {
    "id": "044d9c6a"
   },
   "source": [
    "# 2025 데이터 크리에이터 캠프\n",
    "\n",
    "@PHASE: Mission 2\n",
    "\n",
    "@TEAM: 최후의 인공지능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5ea4cfae13401",
   "metadata": {
    "id": "95b5ea4cfae13401"
   },
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e888eba8b18b17",
   "metadata": {
    "id": "a7e888eba8b18b17"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa704fa81e6072",
   "metadata": {
    "id": "5efa704fa81e6072"
   },
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066676dc",
   "metadata": {
    "id": "066676dc"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e9c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08ea64",
   "metadata": {
    "id": "2b08ea64"
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffd3da4d752be5",
   "metadata": {
    "id": "23ffd3da4d752be5"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e1c723f76fc4f",
   "metadata": {
    "id": "f52e1c723f76fc4f"
   },
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dFJtzYqx3Lbv",
   "metadata": {
    "cellView": "form",
    "id": "dFJtzYqx3Lbv",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetHolder:\n",
    "    train: Dataset = None\n",
    "    valid: Dataset = None\n",
    "    test: Dataset = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        print(f\"INFO: Dataset loaded successfully. Number of samples - \", end='')\n",
    "        if self.train:\n",
    "            print(f\"Train: {len(self.train)}\", end='')\n",
    "        if self.valid:\n",
    "            if self.train: print(', ', end='')\n",
    "            print(f\"Valid: {len(self.valid)}\", end='')\n",
    "        if self.test:\n",
    "            if self.train: print(', ', end='')\n",
    "            print(f\"Test: {len(self.test)}\", end='')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bTLrZlj3Pkh",
   "metadata": {
    "cellView": "form",
    "id": "3bTLrZlj3Pkh",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "# Github Release URL for datasets\n",
    "# This will be removed after the contest ends due to the copyright issue.\n",
    "base_git_path = \"https://github.com/b-re-w/K-ICT_DataCreatorCamp_2025/releases/download/dt/\"\n",
    "\n",
    "\n",
    "class KompsatIndex(Enum):\n",
    "    TRAIN = \"TS_KS.zip\"\n",
    "    VALID = \"VS_KS.zip\"\n",
    "    TRAIN_BBOX = \"TL_KS_BBOX.zip\"\n",
    "    VALID_BBOX = \"VL_KS_BBOX.zip\"\n",
    "    TRAIN_LINE = \"TL_KS_LINE.zip\"\n",
    "    VALID_LINE = \"VL_KS_LINE.zip\"\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        return f\"{base_git_path}{self.value}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gwdD_FuK3EbL",
   "metadata": {
    "cellView": "form",
    "id": "gwdD_FuK3EbL",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import VisionDataset, utils, folder\n",
    "from torchvision.ops import box_convert\n",
    "import torch\n",
    "\n",
    "import traceback\n",
    "from os import path\n",
    "from glob import glob\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional, Callable\n",
    "\n",
    "from json import load as json_load\n",
    "\n",
    "from tqdm.asyncio import tqdm\n",
    "import concurrent.futures\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class KompsatType(Enum):\n",
    "    BBOX = \"bbox\"\n",
    "    LINE = \"line\"\n",
    "\n",
    "\n",
    "class KompsatDataset(VisionDataset):\n",
    "    dataset_name = \"Kompsat\"\n",
    "\n",
    "    @classmethod\n",
    "    async def download_method(cls, url, root, filename):\n",
    "        loop = asyncio.get_event_loop()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            await loop.run_in_executor(executor, utils.download_url, url, root, filename)\n",
    "\n",
    "    @classmethod\n",
    "    async def extract_method(cls, from_path, to_path):\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                await loop.run_in_executor(executor, utils.extract_archive, from_path, to_path)\n",
    "        except FileExistsError as e:\n",
    "            traceback.print_exc()\n",
    "            raise FileExistsError(str(e) + \"\\nPlease use Python 3.13 or later. 3.12 or earlier versions not support unzip over existing directory.\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path] = None,\n",
    "        train: bool = True,\n",
    "        data_type: KompsatType = KompsatType.BBOX,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Kompsat-3/3A dataset for object detection and height estimation.\n",
    "\n",
    "        Args:\n",
    "            root: Dataset root directory\n",
    "            train: True for training set, False for validation set\n",
    "            data_type: BBOX or LINE\n",
    "            transform: Image transforms\n",
    "            target_transform: Mask transforms\n",
    "        \"\"\"\n",
    "        super().__init__(root, transforms=transform, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(self.download(root))\n",
    "\n",
    "        self.root = path.join(root, self.dataset_name)\n",
    "        self.train = train\n",
    "        self.data_type = data_type\n",
    "        split = \"train\" if train else \"val\"\n",
    "        img_dir = path.join(self.root, \"images\", split)\n",
    "        ann_dir = path.join(self.root, \"annotations\", split)\n",
    "        line_dir = path.join(self.root, \"lines\", split)\n",
    "\n",
    "        self.images = sorted(glob(path.join(img_dir, \"*.jpg\")))\n",
    "        self.labels = []\n",
    "        for pth in self.images:\n",
    "            if data_type is KompsatType.BBOX:\n",
    "                annotation_path = path.join(ann_dir, Path(pth).stem + \".json\")\n",
    "                if not path.exists(annotation_path):\n",
    "                    raise FileNotFoundError(f\"Annotation file {annotation_path} does not exist.\")\n",
    "                annotation = list(json_load(open(annotation_path, \"r\", encoding=\"utf-8\")).values())[0]\n",
    "                regions = []\n",
    "                for anno in sorted(annotation['regions'], key=lambda x: int(x['region_attributes']['chi_id'])):\n",
    "                    bbox = anno[\"shape_attributes\"]\n",
    "                    bbox = [bbox[\"x\"], bbox[\"y\"], bbox[\"width\"], bbox[\"height\"]]\n",
    "                    regions.append(dict(\n",
    "                        chi_id=int(anno[\"region_attributes\"][\"chi_id\"]),\n",
    "                        xywh=bbox,\n",
    "                        xyxy=box_convert(torch.tensor(bbox), \"xywh\", \"xyxy\").tolist(),\n",
    "                        cxcywh=box_convert(torch.tensor(bbox), \"xywh\", \"cxcywh\").tolist()\n",
    "                    ))\n",
    "                annotation['regions'] = regions\n",
    "                annotation['file_attributes']['img_width'] = int(annotation['file_attributes']['img_width'])\n",
    "                annotation['file_attributes']['img_height'] = int(annotation['file_attributes']['img_height'])\n",
    "                self.labels.append(annotation)\n",
    "            else:\n",
    "                label_path = path.join(line_dir, Path(pth).stem + \".json\")\n",
    "                if not path.exists(label_path):\n",
    "                    raise FileNotFoundError(f\"Label file {label_path} does not exist.\")\n",
    "                label = list(json_load(open(label_path, \"r\", encoding=\"utf-8\")).values())[0]\n",
    "                regions = []\n",
    "                for ln in sorted(label['regions'], key=lambda x: int(x['region_attributes']['chi_id'])):\n",
    "                    poly = ln[\"shape_attributes\"]\n",
    "                    xyxy = [poly[\"all_points_x\"][0], poly[\"all_points_y\"][0], poly[\"all_points_x\"][1], poly[\"all_points_y\"][1]]\n",
    "                    x1 = max(min(xyxy[0], xyxy[2]) - 1, 0)\n",
    "                    y1 = max(min(xyxy[1], xyxy[3]) - 1, 0)\n",
    "                    w = abs(xyxy[2] - xyxy[0]) + 2\n",
    "                    h = max(abs(xyxy[3] - xyxy[1]), 1) + 2\n",
    "                    regions.append(dict(\n",
    "                        polyline=xyxy,\n",
    "                        xywh=[x1, y1, w, h],\n",
    "                        chi_height=float(ln[\"region_attributes\"]['chi_height_m']),\n",
    "                    ))\n",
    "                label['regions'] = regions\n",
    "                label['file_attributes']['img_width'] = int(label['file_attributes']['img_width'])\n",
    "                label['file_attributes']['img_height'] = int(label['file_attributes']['img_height'])\n",
    "                self.labels.append(label)\n",
    "\n",
    "        assert len(self.images) == len(self.labels), \\\n",
    "            f\"Number of images ({len(self.images)}) and labels ({len(self.labels)}) do not match.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sample from the dataset\"\"\"\n",
    "        # Load image/label using default_loader\n",
    "        image = folder.default_loader(self.images[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    @classmethod\n",
    "    async def download(cls, root: str):\n",
    "        dataset_root = path.join(root, cls.dataset_name)\n",
    "        if path.exists(dataset_root):  # If the dataset directory already exists, skip download\n",
    "            return\n",
    "\n",
    "        data_list = [\n",
    "            KompsatIndex.TRAIN, KompsatIndex.VALID,\n",
    "            KompsatIndex.TRAIN_BBOX, KompsatIndex.VALID_BBOX,\n",
    "            KompsatIndex.TRAIN_LINE, KompsatIndex.VALID_LINE\n",
    "        ]\n",
    "\n",
    "        print(f\"INFO: Downloading '{cls.dataset_name}' from server to {root}...\")\n",
    "        routines = []\n",
    "        for data in data_list:\n",
    "            if path.isfile(path.join(root, data.value)):\n",
    "                print(f\"INFO: Dataset archive {data.value} found in the root directory. Skipping download.\")\n",
    "                continue\n",
    "\n",
    "            routines.append(cls.download_method(data.url, root=root, filename=data.value))\n",
    "        await tqdm.gather(*routines, desc=\"Downloading files\")\n",
    "\n",
    "        print(f\"INFO: Extracting '{cls.dataset_name}' dataset...\")\n",
    "        routines = []\n",
    "        img_dir, anno_dir, line_dir = path.join(dataset_root, \"images\"), path.join(dataset_root, \"annotations\"), path.join(dataset_root, \"lines\")\n",
    "        as_train, as_valid = lambda d: path.join(d, \"train\"), lambda d: path.join(d, \"val\")\n",
    "        routines.extend((\n",
    "            cls.extract_method(path.join(root, KompsatIndex.TRAIN.value), to_path=as_train(img_dir)),\n",
    "            cls.extract_method(path.join(root, KompsatIndex.VALID.value), to_path=as_valid(img_dir)),\n",
    "            cls.extract_method(path.join(root, KompsatIndex.TRAIN_BBOX.value), to_path=as_train(anno_dir)),\n",
    "            cls.extract_method(path.join(root, KompsatIndex.VALID_BBOX.value), to_path=as_valid(anno_dir)),\n",
    "            cls.extract_method(path.join(root, KompsatIndex.TRAIN_LINE.value), to_path=as_train(line_dir)),\n",
    "            cls.extract_method(path.join(root, KompsatIndex.VALID_LINE.value), to_path=as_valid(line_dir)),\n",
    "        ))\n",
    "        await tqdm.gather(*routines, desc=\"Extracting files\")\n",
    "\n",
    "\n",
    "class KompsatDatasetForObjectDetection(KompsatDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path] = None,\n",
    "        train: bool = True,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        super().__init__(root, train, KompsatType.BBOX, transform, target_transform)\n",
    "\n",
    "\n",
    "class KompsatDatasetForHeightRegression(KompsatDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path] = None,\n",
    "        train: bool = True,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        super().__init__(root, train, KompsatType.LINE, transform, target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973243d0d0632803",
   "metadata": {
    "cellView": "form",
    "id": "973243d0d0632803",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "kompstats = DatasetHolder(\n",
    "    train=KompsatDatasetForHeightRegression(root=DATA_ROOT, train=True),\n",
    "    valid=KompsatDatasetForHeightRegression(root=DATA_ROOT, train=False)\n",
    ")\n",
    "kompstats.test = kompstats.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1973a6e3a7a4f27",
   "metadata": {
    "id": "e1973a6e3a7a4f27"
   },
   "outputs": [],
   "source": [
    "kompstats.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d102cd1b5fdcc1",
   "metadata": {
    "id": "49d102cd1b5fdcc1",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rgb_image, annotation = kompstats.train[0]\n",
    "fig, axes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "axes.imshow(rgb_image)\n",
    "axes.set_title('Image')\n",
    "axes.axis('off')\n",
    "\n",
    "for region in annotation['regions']:\n",
    "    x1, y1, w, h = region['xywh']\n",
    "    rect = plt.Rectangle((x1, y1), w, h, fill=False, edgecolor='red', linewidth=2)\n",
    "    axes.add_patch(rect)\n",
    "\n",
    "    polyline = region['polyline']\n",
    "    xs = [polyline[0], polyline[2]]\n",
    "    ys = [polyline[1], polyline[3]]\n",
    "    axes.plot(xs, ys, color='blue', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf5ad3ca11f80de",
   "metadata": {
    "id": "acf5ad3ca11f80de"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def target_transform(data):\n",
    "    regions = data['regions']\n",
    "    polylines = []\n",
    "    heights = []\n",
    "    for region in regions:\n",
    "        polyline = region['polyline']  # region['xywh']\n",
    "        height = region['chi_height']\n",
    "        polylines.append(polyline)\n",
    "        heights.append(height)\n",
    "    return torch.tensor(polylines), torch.tensor(heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10cf8f34ed035",
   "metadata": {
    "id": "cc10cf8f34ed035"
   },
   "outputs": [],
   "source": [
    "kompstats.train.transform = transform\n",
    "kompstats.valid.transform = transform\n",
    "kompstats.train.target_transform = target_transform\n",
    "kompstats.valid.target_transform = target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95557966a7fedc94",
   "metadata": {
    "id": "95557966a7fedc94"
   },
   "outputs": [],
   "source": [
    "kompstats.train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bdb9913b15b4d",
   "metadata": {
    "id": "42bdb9913b15b4d"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012c108dcc062bb",
   "metadata": {
    "id": "d012c108dcc062bb"
   },
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 50, 100, 100  # A100\n",
    "BATCH_SIZE = 12, 64, 64\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a96098aa913cb",
   "metadata": {
    "id": "708a96098aa913cb"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    coords, heights = zip(*labels)\n",
    "    return torch.stack(images), coords, heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b6700646635a9",
   "metadata": {
    "id": "404b6700646635a9"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(kompstats.train, batch_size=BATCH_SIZE[0], shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(kompstats.valid, batch_size=BATCH_SIZE[1], shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(kompstats.test, batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4053c037f244da",
   "metadata": {
    "id": "4e4053c037f244da"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kweTWpoab0xz",
   "metadata": {
    "id": "kweTWpoab0xz"
   },
   "outputs": [],
   "source": [
    "class NaiveRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_dim = 512\n",
    "        self.image_encoder = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.image_encoder.fc = nn.Linear(self.image_encoder.fc.in_features, self.feature_dim)\n",
    "\n",
    "        self.keypoint_encoder = nn.Sequential(\n",
    "            nn.Linear(4, 64),     # [kpt_x1, kpt_y1, kpt_x2, kpt_y2]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256)\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(512 + 256, 512),  # 이미지 특징 + 키포인트 특징\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)    # 높이 출력\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, polylines):\n",
    "        outputs = self.image_encoder(pixel_values.to(device))\n",
    "        results = []\n",
    "\n",
    "        for hidden_states, polyline in zip(outputs, polylines):\n",
    "            hidden_states = hidden_states.unsqueeze(0)\n",
    "            if polyline.shape[0] > 1:\n",
    "                hidden_states = hidden_states.expand(polyline.shape[0], -1)\n",
    "\n",
    "            kpt_features = self.keypoint_encoder(polyline.float().to(device))\n",
    "            combined_features = torch.cat((hidden_states, kpt_features), dim=1)\n",
    "            height = self.predictor(combined_features)\n",
    "\n",
    "            results.append(height.reshape(-1))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bab943-d1df-46f0-a687-2f085caf5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGeometricRegression(nn.Module):\n",
    "    def __init__(self, img_size=224):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.feature_dim = 256  # 이미지 특징 차원 축소\n",
    "        \n",
    "        # 이미지 인코더 (가벼워짐)\n",
    "        self.image_encoder = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.image_encoder.fc = nn.Linear(self.image_encoder.fc.in_features, self.feature_dim)\n",
    "\n",
    "        # 키포인트 인코더 (축소)\n",
    "        self.keypoint_encoder = nn.Sequential(\n",
    "            nn.Linear(4, 32),     # [kpt_x1, kpt_y1, kpt_x2, kpt_y2]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64)\n",
    "        )\n",
    "\n",
    "        # 기하학적 특징 인코더 (핵심 추가)\n",
    "        self.geometric_encoder = nn.Sequential(\n",
    "            nn.Linear(6, 32),     # [pixel_dist, angle, center_x, center_y, aspect_ratio, norm_dist]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128)\n",
    "        )\n",
    "\n",
    "        # 기하학적 높이 예측기 (물리적 계산 기반)\n",
    "        self.geometric_predictor = nn.Sequential(\n",
    "            nn.Linear(1, 16),     # 픽셀 거리만 입력\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)      # 기하학적 높이 추정\n",
    "        )\n",
    "\n",
    "        # 최종 융합 예측기 (가중치를 기하학적 쪽에 더 주기)\n",
    "        self.fusion_predictor = nn.Sequential(\n",
    "            nn.Linear(256 + 64 + 128 + 1, 256),  # 이미지 + 키포인트 + 기하학적 + 기하학적높이\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)     # 최종 높이 출력\n",
    "        )\n",
    "\n",
    "    def calculate_geometric_features(self, polyline):\n",
    "        \"\"\"키포인트로부터 기하학적 특징 계산\"\"\"\n",
    "        batch_size = polyline.shape[0]\n",
    "        geometric_features = torch.zeros(batch_size, 6, device=polyline.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            kpt_x1, kpt_y1, kpt_x2, kpt_y2 = polyline[i]\n",
    "            \n",
    "            # 1. 픽셀 거리 계산\n",
    "            pixel_distance = torch.sqrt((kpt_x2 - kpt_x1)**2 + (kpt_y2 - kpt_y1)**2)\n",
    "            \n",
    "            # 2. 키포인트 간 각도\n",
    "            angle = torch.atan2(kpt_y2 - kpt_y1, kpt_x2 - kpt_x1)\n",
    "            \n",
    "            # 3. 중심점 좌표 (정규화)\n",
    "            center_x = (kpt_x1 + kpt_x2) / 2 / self.img_size\n",
    "            center_y = (kpt_y1 + kpt_y2) / 2 / self.img_size\n",
    "            \n",
    "            # 4. 종횡비 (세로/가로)\n",
    "            height_diff = torch.abs(kpt_y2 - kpt_y1)\n",
    "            width_diff = torch.abs(kpt_x2 - kpt_x1)\n",
    "            aspect_ratio = height_diff / (width_diff + 1e-8)\n",
    "            \n",
    "            # 5. 정규화된 거리 (이미지 대각선 대비)\n",
    "            diagonal = torch.sqrt(torch.tensor(self.img_size**2 + self.img_size**2, device=polyline.device))\n",
    "            normalized_distance = pixel_distance / diagonal\n",
    "            \n",
    "            geometric_features[i] = torch.stack([\n",
    "                pixel_distance, angle, center_x, center_y, aspect_ratio, normalized_distance\n",
    "            ])\n",
    "        \n",
    "        return geometric_features\n",
    "\n",
    "    def geometric_height_estimation(self, pixel_distance):\n",
    "        \"\"\"기하학적 높이 추정 (물리적 계산 시뮬레이션)\"\"\"\n",
    "        # 간단한 스케일 변환 (실제로는 GSD, 카메라 파라미터 등을 고려)\n",
    "        # 이 부분은 위성 이미지의 메타데이터가 있다면 더 정확하게 계산 가능\n",
    "        normalized_distance = pixel_distance / self.img_size\n",
    "        geometric_height = self.geometric_predictor(normalized_distance.unsqueeze(-1))\n",
    "        return geometric_height\n",
    "\n",
    "    def forward(self, pixel_values, polylines):\n",
    "        # 이미지 인코딩\n",
    "        image_features = self.image_encoder(pixel_values.to(device))\n",
    "        results = []\n",
    "\n",
    "        for hidden_states, polyline in zip(image_features, polylines):\n",
    "            hidden_states = hidden_states.unsqueeze(0)\n",
    "            if polyline.shape[0] > 1:\n",
    "                hidden_states = hidden_states.expand(polyline.shape[0], -1)\n",
    "\n",
    "            # 1. 키포인트 특징 추출\n",
    "            kpt_features = self.keypoint_encoder(polyline.float().to(device))\n",
    "            \n",
    "            # 2. 기하학적 특징 계산\n",
    "            geometric_features = self.calculate_geometric_features(polyline.float().to(device))\n",
    "            geo_features = self.geometric_encoder(geometric_features)\n",
    "            \n",
    "            # 3. 기하학적 높이 추정\n",
    "            pixel_distances = geometric_features[:, 0]  # 첫 번째 특징이 픽셀 거리\n",
    "            geo_heights = self.geometric_height_estimation(pixel_distances)\n",
    "            \n",
    "            # 4. 모든 특징 융합\n",
    "            combined_features = torch.cat((\n",
    "                hidden_states,      # 이미지 특징\n",
    "                kpt_features,       # 키포인트 특징  \n",
    "                geo_features,       # 기하학적 특징\n",
    "                geo_heights         # 기하학적 높이 추정\n",
    "            ), dim=1)\n",
    "            \n",
    "            # 5. 최종 높이 예측\n",
    "            height = self.fusion_predictor(combined_features)\n",
    "            results.append(height.reshape(-1))\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8336b-5049-4945-a7cd-937b1200e864",
   "metadata": {
    "id": "AqqfaXOuPj8J"
   },
   "outputs": [],
   "source": [
    "class GeometricPriorityRegression(nn.Module):\n",
    "    def __init__(self, img_size=224):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # 기하학적 높이 예측기 (메인)\n",
    "        self.geometric_predictor = nn.Sequential(\n",
    "            nn.Linear(4, 64),     # [pixel_dist, norm_dist, angle, aspect_ratio]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)      # 기하학적 높이 추정\n",
    "        )\n",
    "\n",
    "        # 이미지 맥락 보정기 (보조)\n",
    "        self.image_encoder = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
    "        self.image_encoder.fc = nn.Linear(self.image_encoder.fc.in_features, 128)\n",
    "\n",
    "        # 최종 보정기 (기하학적 결과를 미세 조정)\n",
    "        self.corrector = nn.Sequential(\n",
    "            nn.Linear(128 + 1, 64),  # 이미지 특징 + 기하학적 높이\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)         # 보정값 출력\n",
    "        )\n",
    "\n",
    "    def calculate_geometric_features(self, polyline):\n",
    "        \"\"\"핵심 기하학적 특징만 계산\"\"\"\n",
    "        batch_size = polyline.shape[0]\n",
    "        features = torch.zeros(batch_size, 4, device=polyline.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            kpt_x1, kpt_y1, kpt_x2, kpt_y2 = polyline[i]\n",
    "\n",
    "            # 픽셀 거리\n",
    "            pixel_distance = torch.sqrt((kpt_x2 - kpt_x1)**2 + (kpt_y2 - kpt_y1)**2)\n",
    "\n",
    "            # 정규화된 거리\n",
    "            diagonal = torch.sqrt(torch.tensor(2 * self.img_size**2, device=polyline.device))\n",
    "            normalized_distance = pixel_distance / diagonal\n",
    "\n",
    "            # 각도\n",
    "            angle = torch.atan2(kpt_y2 - kpt_y1, kpt_x2 - kpt_x1)\n",
    "\n",
    "            # 종횡비\n",
    "            height_diff = torch.abs(kpt_y2 - kpt_y1)\n",
    "            width_diff = torch.abs(kpt_x2 - kpt_x1)\n",
    "            aspect_ratio = height_diff / (width_diff + 1e-8)\n",
    "\n",
    "            features[i] = torch.stack([pixel_distance, normalized_distance, angle, aspect_ratio])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def forward(self, pixel_values, polylines):\n",
    "        # 이미지 전역 특징 (맥락 정보)\n",
    "        image_features = self.image_encoder(pixel_values.to(device))\n",
    "        results = []\n",
    "\n",
    "        for hidden_states, polyline in zip(image_features, polylines):\n",
    "            hidden_states = hidden_states.unsqueeze(0)\n",
    "            if polyline.shape[0] > 1:\n",
    "                hidden_states = hidden_states.expand(polyline.shape[0], -1)\n",
    "\n",
    "            # 1. 기하학적 특징 계산 및 높이 추정 (메인)\n",
    "            geometric_features = self.calculate_geometric_features(polyline.float().to(device))\n",
    "            geometric_height = self.geometric_predictor(geometric_features)\n",
    "\n",
    "            # 2. 이미지 맥락을 통한 보정 (보조)\n",
    "            correction_input = torch.cat((hidden_states, geometric_height), dim=1)\n",
    "            correction = self.corrector(correction_input)\n",
    "\n",
    "            # 3. 최종 높이 = 기하학적 높이 + 보정값\n",
    "            final_height = geometric_height + correction\n",
    "\n",
    "            results.append(final_height.reshape(-1))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda135661c18828",
   "metadata": {
    "id": "3eda135661c18828",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model = GeometricPriorityRegression(img_size=224)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfdb161ae65bd5",
   "metadata": {
    "id": "53dfdb161ae65bd5"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36691d14503ca21",
   "metadata": {
    "id": "b36691d14503ca21"
   },
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 300\n",
    "LEARNING_RATE = 1e-4, 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335029cddea84e1",
   "metadata": {
    "id": "335029cddea84e1"
   },
   "outputs": [],
   "source": [
    "classifier = nn.HuberLoss(delta=1.0)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0], weight_decay=0.05)\n",
    "scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=LEARNING_RATE[1], max_lr=LEARNING_RATE[0], step_size_up=50, gamma=0.95)\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=LEARNING_RATE[1])\n",
    "#scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LEARNING_RATE[0], steps_per_epoch=len(train_loader), epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e7ecd9ece4612",
   "metadata": {
    "id": "c66e7ecd9ece4612"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "while gc.collect():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Running Epochs\"):\n",
    "    train_loss, valid_loss = [], []\n",
    "    train_rmse, valid_rmse = [], []\n",
    "\n",
    "    model.train()\n",
    "    train_bar = tqdm(total=int(len(kompstats.train)/BATCH_SIZE[0]+0.5), desc=f\"Training for {epoch+1}/{EPOCHS}\")\n",
    "    for inputs, coords, heights in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(inputs, coords)\n",
    "        all_preds = torch.cat([pred.flatten() for pred in preds])\n",
    "        all_heights = torch.cat([height.flatten() for height in heights]).to(device).to(all_preds.dtype)\n",
    "\n",
    "        losses = classifier(all_preds, all_heights)\n",
    "        rmse = torch.sqrt(nn.functional.mse_loss(all_preds, all_heights)).item()\n",
    "        if rmse >= 1000 or (epoch >= 50 and rmse >= 10) \\\n",
    "                or (epoch >= 100 and rmse >= 7) or (epoch >= 150 and rmse >= 5) \\\n",
    "                or (epoch >= 200 and rmse >= 3) or (epoch >= 250 and rmse >= 2):  # 이상치 로스 업데이트 안함\n",
    "            train_bar.update(1); continue\n",
    "        losses.backward()\n",
    "\n",
    "        train_loss.append(losses.item())\n",
    "        train_rmse.append(rmse)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_bar.update(1)\n",
    "        train_bar.set_postfix({\"Loss\": f\"{losses.item():.6f}\", \"RMSE(m)\": f\"{rmse:.3f}\", \"LR\": f\"{optimizer.param_groups[0]['lr']:.1e}\"})\n",
    "    train_bar.set_postfix({\"Loss\": f\"{sum(train_loss)/len(train_loss):.6f}\", \"RMSE(m)\": f\"{sum(train_rmse)/len(train_rmse):.3f}\", \"LR\": f\"{optimizer.param_groups[0]['lr']:.1e}\"})\n",
    "    train_bar.close()\n",
    "\n",
    "    model.eval()\n",
    "    valid_bar = tqdm(total=int(len(kompstats.valid)/BATCH_SIZE[1]+0.5), desc=f\"Validating for {epoch+1}/{EPOCHS}\")\n",
    "    with torch.inference_mode():\n",
    "        for inputs, coords, heights in valid_loader:\n",
    "            preds = model(inputs, coords)\n",
    "            all_preds = torch.cat([pred.flatten() for pred in preds])\n",
    "            all_heights = torch.cat([height.flatten() for height in heights]).to(device).to(all_preds.dtype)\n",
    "\n",
    "            losses = classifier(all_preds, all_heights)\n",
    "            rmse = torch.sqrt(nn.functional.mse_loss(all_preds, all_heights))\n",
    "\n",
    "            valid_loss.append(losses.item())\n",
    "            valid_rmse.append(rmse.item())\n",
    "            valid_bar.update(1)\n",
    "\n",
    "    valid_bar.set_postfix({\"Loss\": f\"{sum(valid_loss)/len(valid_loss):.6f}\", \"RMSE(m)\": f\"{sum(valid_rmse)/len(valid_rmse):.3f}\"})\n",
    "    valid_bar.close()\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0aace760f618d0",
   "metadata": {
    "id": "6f0aace760f618d0"
   },
   "source": [
    " ## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009fd42-2055-46af-b04a-c12157ad4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_rmse = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, coords, heights in tqdm(data_loader):\n",
    "            preds = model(inputs, coords)\n",
    "            all_preds = torch.cat([pred.flatten() for pred in preds])\n",
    "            all_heights = torch.cat([height.flatten() for height in heights]).to(device).to(all_preds.dtype)\n",
    "\n",
    "            rmse = torch.sqrt(nn.functional.mse_loss(all_preds, all_heights))\n",
    "            total_rmse += rmse.item() * len(all_heights)\n",
    "            total_samples += len(all_heights)\n",
    "\n",
    "    return total_rmse / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4780dd-ab63-4ca2-ab0c-3f7a04049a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Final Evaluation ===\")\n",
    "test_rmse = evaluate(model, test_loader)\n",
    "print(f\"Test RSME: {test_rmse:.3f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da290d2-44b1-4557-b7d4-5ea48b279308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
