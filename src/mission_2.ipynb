{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044d9c6a",
   "metadata": {},
   "source": [
    "# 2025 데이터 크리에이터 캠프\n",
    "\n",
    "@PHASE: Mission 2\n",
    "\n",
    "@TEAM: 최후의 인공지능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5ea4cfae13401",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e888eba8b18b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:46:53.557924Z",
     "start_time": "2025-09-04T16:46:51.622630Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa704fa81e6072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:46:53.615858Z",
     "start_time": "2025-09-04T16:46:53.593756Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([f\"{i+DEVICE_NUM}\" for i in range(0, ADDITIONAL_GPU+1)])\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066676dc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08ea64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:47:42.774227Z",
     "start_time": "2025-09-04T16:46:53.639288Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "from creator_camp.datasets import KompsatDatasetForHeightRegression, DatasetHolder\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "#import wandb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffd3da4d752be5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:47:45.086271Z",
     "start_time": "2025-09-04T16:47:43.254616Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda\")  # torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215d175116b3ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:47:45.165838Z",
     "start_time": "2025-09-04T16:47:45.162414Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"Mission_2\"\n",
    "RUN_NAME = \"RT-DETR\"\n",
    "\n",
    "# WandB Initialization\n",
    "environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "#wandb.init(project=PROJECT_NAME, name=RUN_NAME, mode=\"offline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e1c723f76fc4f",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973243d0d0632803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:52.810151Z",
     "start_time": "2025-09-04T16:47:45.175547Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "kompstats = DatasetHolder(train=KompsatDatasetForHeightRegression(root=DATA_ROOT, train=True), valid=KompsatDatasetForHeightRegression(root=DATA_ROOT, train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1973a6e3a7a4f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:53.256025Z",
     "start_time": "2025-09-04T16:49:53.052693Z"
    }
   },
   "outputs": [],
   "source": [
    "kompstats.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d102cd1b5fdcc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:53.554414Z",
     "start_time": "2025-09-04T16:49:53.292212Z"
    }
   },
   "outputs": [],
   "source": [
    "rgb_image, annotation = kompstats.train[0]\n",
    "fig, axes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "axes.imshow(rgb_image)\n",
    "axes.set_title('Image')\n",
    "axes.axis('off')\n",
    "\n",
    "for region in annotation['regions']:\n",
    "    x1, y1, w, h = region['polyline_xywh']\n",
    "    rect = plt.Rectangle((x1, y1), w, h, fill=False, edgecolor='red', linewidth=2)\n",
    "    axes.add_patch(rect)\n",
    "\n",
    "    polyline = region['polyline']\n",
    "    xs = [polyline[0], polyline[2]]\n",
    "    ys = [polyline[1], polyline[3]]\n",
    "    axes.plot(xs, ys, color='blue', linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752cebd36e5c809",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d124eb51bc123a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:53.616987Z",
     "start_time": "2025-09-04T16:49:53.608847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 4, 8, 8, 4  # Local\n",
    "#BATCH_SIZE = 32, 64, 64, 32  # A100\n",
    "\n",
    "# Dataset Configs\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230530fa899d5c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:53.671141Z",
     "start_time": "2025-09-04T16:49:53.662519Z"
    }
   },
   "outputs": [],
   "source": [
    "ORIGIN_SIZE = 512\n",
    "\n",
    "def collate_fn(batch, preprocessor):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = []\n",
    "    heights = []\n",
    "\n",
    "    for annotations in [item[1] for item in batch]:\n",
    "        ann_list = []\n",
    "        height_list = []\n",
    "        for reg in annotations['regions']:\n",
    "            _, _, w, h = reg['polyline_xywh']\n",
    "            ann_list.append(dict(\n",
    "                bbox=reg['polyline_xywh'],\n",
    "                category_id=1,\n",
    "                area=w * h,\n",
    "                iscrowd=0\n",
    "            ))\n",
    "            height_list.append(min(reg['chi_height'] / ORIGIN_SIZE, 1.0))\n",
    "        targets.append(dict(image_id=len(targets), annotations=ann_list))\n",
    "        heights.append(height_list)\n",
    "\n",
    "    processed = preprocessor(images=images, annotations=targets, return_tensors=\"pt\")  # normalizes bbox to [0, 1]\n",
    "\n",
    "    for label, h in zip(processed['labels'], heights):\n",
    "        h = torch.tensor(h, dtype=torch.float32).view(-1, 1)\n",
    "        assert label['boxes'].shape[0] == h.shape[0], f\"label['boxes'].shape[0] = {label['boxes'].shape[0]}, h.shape[0] = {h.shape[0]}, original data = {[item[1] for item in batch]}\"\n",
    "        label['boxes'] = torch.cat([label['boxes'], h], dim=1)  # check if preprocessor removed any bbox\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4053c037f244da",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf1c3ff5b91ae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:53.881249Z",
     "start_time": "2025-09-04T16:49:53.686224Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RTDetrImageProcessorFast, RTDetrConfig\n",
    "from transformers.image_utils import AnnotationFormat\n",
    "from creator_camp.models.rt_detr import RTDetrForObjectDetectionWithHeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b44b1439d76ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:54.534220Z",
     "start_time": "2025-09-04T16:49:53.907854Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = NUM_CLASSES + 1\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 640\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": 640, \"width\": 640}\n",
    "reference_preprocessor.do_resize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b05b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:54.640856Z",
     "start_time": "2025-09-04T16:49:54.547330Z"
    }
   },
   "outputs": [],
   "source": [
    "collate_fn([kompstats.train[0], kompstats.train[1]], reference_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44373f47d600fb82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:57.618269Z",
     "start_time": "2025-09-04T16:49:54.695803Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RTDetrForObjectDetectionWithHeight.from_pretrained(reference_model_id, config=reference_config, torch_dtype=torch.float32, ignore_mismatched_sizes=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07444e7f1c02324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:57.656598Z",
     "start_time": "2025-09-04T16:49:57.648221Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.image_transforms import center_to_corners_format\n",
    "\n",
    "def post_process_object_detection(\n",
    "    self,\n",
    "    outputs,\n",
    "    threshold: float = 0.5,\n",
    "    target_sizes: list[tuple] = None,\n",
    "    use_focal_loss: bool = True,\n",
    "):\n",
    "    out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n",
    "    # convert from relative cxcywh to absolute xyxy\n",
    "    boxes, heights = center_to_corners_format(out_bbox[:, :, :4]), out_bbox[:, :, 4:]\n",
    "    if target_sizes is not None:\n",
    "        if len(out_logits) != len(target_sizes):\n",
    "            raise ValueError(\n",
    "                \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n",
    "            )\n",
    "        if isinstance(target_sizes, list):\n",
    "            img_h, img_w = torch.as_tensor(target_sizes).unbind(1)\n",
    "        else:\n",
    "            img_h, img_w = target_sizes.unbind(1)\n",
    "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n",
    "        boxes = boxes * scale_fct[:, None, :]\n",
    "        height_scale = img_h.unsqueeze(1).to(heights.device)\n",
    "        heights = heights * height_scale[:, None, :]\n",
    "\n",
    "    num_top_queries = out_logits.shape[1]\n",
    "    num_classes = out_logits.shape[2]\n",
    "\n",
    "    if use_focal_loss:\n",
    "        scores = torch.nn.functional.sigmoid(out_logits)\n",
    "        scores, index = torch.topk(scores.flatten(1), num_top_queries, axis=-1)\n",
    "        labels = index % num_classes\n",
    "        index = index // num_classes\n",
    "        boxes = boxes.gather(dim=1, index=index.unsqueeze(-1).repeat(1, 1, boxes.shape[-1]))\n",
    "        heights = heights.gather(dim=1, index=index.unsqueeze(-1).repeat(1, 1, heights.shape[-1]))\n",
    "    else:\n",
    "        scores = torch.nn.functional.softmax(out_logits)[:, :, :-1]\n",
    "        scores, labels = scores.max(dim=-1)\n",
    "        if scores.shape[1] > num_top_queries:\n",
    "            scores, index = torch.topk(scores, num_top_queries, dim=-1)\n",
    "            labels = torch.gather(labels, dim=1, index=index)\n",
    "            boxes = torch.gather(boxes, dim=1, index=index.unsqueeze(-1).tile(1, 1, boxes.shape[-1]))\n",
    "            heights = heights.gather(heights, dim=1, index=index.unsqueeze(-1).tile(1, 1, heights.shape[-1]))\n",
    "\n",
    "    results = []\n",
    "    for score, label, box, height in zip(scores, labels, boxes, heights):\n",
    "        mask = score > threshold\n",
    "        results.append(\n",
    "            {\n",
    "                \"scores\": score[mask],\n",
    "                \"labels\": label[mask],\n",
    "                \"boxes\": box[mask],\n",
    "                \"heights\": height[mask]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "RTDetrImageProcessorFast.post_process_object_detection = post_process_object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e913edc10cfef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:49:57.681059Z",
     "start_time": "2025-09-04T16:49:57.671343Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import EvalPrediction\n",
    "from torchvision.ops import box_convert\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "def map_compute_metrics(preprocessor=reference_preprocessor, threshold=0.0):\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    height_metric = []\n",
    "    mse_loss = torch.nn.functional.mse_loss\n",
    "    post_process = preprocessor.post_process_object_detection\n",
    "\n",
    "    def calc(eval_pred: EvalPrediction, compute_result=False):\n",
    "        nonlocal map_metric\n",
    "\n",
    "        if compute_result:\n",
    "            m_ap = map_metric.compute()\n",
    "            map_metric.reset()\n",
    "            height_rmse = []\n",
    "            for preds, labels in zip(*height_metric):\n",
    "                for label in labels:\n",
    "                    rmse = torch.sqrt(mse_loss(preds, label.expand(preds.shape[0], 1)))\n",
    "                    height_rmse.append(torch.min(rmse))\n",
    "            height_metric.clear()\n",
    "\n",
    "            return {\n",
    "                \"RMSE\": torch.mean(torch.stack(height_rmse)).item(),\n",
    "                \"mAP@0.50:0.95\": m_ap.map50_95,\n",
    "                \"mAP@0.50\": m_ap.map50,\n",
    "                \"mAP@0.75\": m_ap.map75\n",
    "            }\n",
    "        else:\n",
    "            preds = ModelOutput(*eval_pred.predictions[1:3])\n",
    "            labels = eval_pred.label_ids\n",
    "            sizes = [label['orig_size'].cpu().tolist() for label in labels]\n",
    "\n",
    "            results = post_process(preds, target_sizes=sizes, threshold=threshold)\n",
    "            predictions = [Detections.from_transformers(result) for result in results]\n",
    "            targets = [Detections(\n",
    "                xyxy=box_convert(label['boxes'][:, :4], \"cxcywh\", \"xyxy\").detach().cpu().numpy(),\n",
    "                class_id=label['class_labels'].detach().cpu().numpy(),\n",
    "            ) for label in labels]\n",
    "\n",
    "            map_metric.update(predictions=predictions, targets=targets)\n",
    "\n",
    "            height_preds = [result['heights'] for result in results]\n",
    "            height_targets = [label['boxes'][:, 4:] for label in labels]\n",
    "\n",
    "            if len(height_metric) != 2:\n",
    "                height_metric.extend([[], []])\n",
    "\n",
    "            height_metric[0].extend(height_preds)\n",
    "            height_metric[1].extend(height_targets)\n",
    "            return {}\n",
    "\n",
    "    return calc, map_metric, height_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfdb161ae65bd5",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36691d14503ca21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:50:01.250298Z",
     "start_time": "2025-09-04T16:49:57.690703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 30\n",
    "REAL_BATCH = BATCH_SIZE[-1]\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.5,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE[0],\n",
    "    per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "    gradient_accumulation_steps=REAL_BATCH//BATCH_SIZE[0],\n",
    "    eval_accumulation_steps=BATCH_SIZE[1],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    "    #metric_for_best_model=\"mAP@0.50\",\n",
    "    #greater_is_better=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    #report_to=\"wandb\",\n",
    "    output_dir=\"./results/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    #run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5273c531e2c1858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:50:01.533432Z",
     "start_time": "2025-09-04T16:50:01.308263Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "compute_metrics, compute_results, height_metric = map_compute_metrics(preprocessor=reference_preprocessor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=kompstats.train,\n",
    "    eval_dataset=kompstats.valid,\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2a1e328759ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T16:50:01.546633Z",
     "start_time": "2025-09-04T16:50:01.542840Z"
    }
   },
   "outputs": [],
   "source": [
    "def start_train():\n",
    "    accelerator = Accelerator()\n",
    "    compute_results.reset()\n",
    "    height_metric.clear()\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8127090f2f63c77",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-09-04T16:50:01.553880Z"
    }
   },
   "outputs": [],
   "source": [
    "if ADDITIONAL_GPU:\n",
    "    notebook_launcher(start_train, args=(), num_processes=ADDITIONAL_GPU)\n",
    "else:\n",
    "    start_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7120551d2adf2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9029e04c31a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = height_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc5659511f981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80557066ba7d024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0aace760f618d0",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b420442c9a00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff6bbc7f16d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 31100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809162a05dd93596",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = RTDetrForObjectDetection.from_pretrained(f\"{training_args.output_dir}/checkpoint-{checkpoint}/\", torch_dtype=torch.float32, return_dict=True, local_files_only=True)\n",
    "    model.to(device)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3a7e501e093d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creator-camp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
